{
  "snapshot_time": "2026-02-22T12:57:36.731085+00:00",
  "repository": "chtmp223/papers-feed",
  "objects": {
    "paper:nber.w34777": {
      "data": {
        "rating": "novote",
        "doi": "10.3386/w34777",
        "timestamp": "2026-02-08T01:07:44.172Z",
        "abstract": "Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.",
        "url": "https://www.nber.org/papers/w34777",
        "authors": "Imke Reimers, Joel Waldfogel",
        "publishedDate": "2026/02/02",
        "title": "AI and the Quantity and Quality of Creative Products: Have LLMs Boosted Creation of Valuable Books?",
        "sourceId": "nber",
        "tags": [
          "Imke Reimers",
          "Joel Waldfogel"
        ],
        "paperId": "w34777",
        "journalName": "NBER Working Papers",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 12,
        "object_id": "paper:nber.w34777",
        "created_at": "2026-02-08T01:07:44+00:00",
        "updated_at": "2026-02-08T01:08:12+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.05125": {
      "data": {
        "rating": "novote",
        "doi": "",
        "timestamp": "2026-02-08T01:23:36.193Z",
        "abstract": "Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.",
        "url": "https://arxiv.org/abs/2602.05125",
        "authors": "William F. Shen, Xinchi Qiu, Chenxi Whitehouse, Lisa Alazraki, Shashwat Goel, Francesco Barbieri, Timon Willi, Akhil Mathur, Ilias Leontiadis",
        "publishedDate": "2026/02/04",
        "title": "Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks",
        "sourceId": "arxiv",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "paperId": "2602.05125",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 13,
        "object_id": "paper:arxiv.2602.05125",
        "created_at": "2026-02-08T01:23:36+00:00",
        "updated_at": "2026-02-08T01:23:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.21996": {
      "data": {
        "rating": "novote",
        "doi": "",
        "timestamp": "2026-02-08T02:10:02.187Z",
        "abstract": "While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.",
        "url": "https://arxiv.org/abs/2601.21996",
        "authors": "Jianhui Chen, Yuzhang Luo, Liangming Pan",
        "publishedDate": "2026/01/29",
        "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
        "sourceId": "arxiv",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "paperId": "2601.21996",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 15,
        "object_id": "paper:arxiv.2601.21996",
        "created_at": "2026-02-08T02:10:02+00:00",
        "updated_at": "2026-02-08T02:10:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.12501": {
      "data": {
        "rating": "novote",
        "doi": "",
        "timestamp": "2026-02-08T02:09:57.564Z",
        "abstract": "Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to deploy the latest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some level of quantitative background. The book starts with the origins of RLHF -- both in recent literature and in a convergence of disparate fields of science in economics, philosophy, and optimal control. We then set the stage with definitions, problem formulation, data collection, and other common math used in the literature. The core of the book details every optimization stage in using RLHF, from starting with instruction tuning to training a reward model and finally all of rejection sampling, reinforcement learning, and direct alignment algorithms. The book concludes with advanced topics -- understudied research questions in synthetic data and evaluation -- and open questions for the field.",
        "url": "https://arxiv.org/abs/2504.12501",
        "authors": "Nathan Lambert",
        "publishedDate": "2025/04/16",
        "title": "Reinforcement Learning from Human Feedback",
        "sourceId": "arxiv",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "paperId": "2504.12501",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 14,
        "object_id": "paper:arxiv.2504.12501",
        "created_at": "2026-02-08T02:09:57+00:00",
        "updated_at": "2026-02-08T02:10:16+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.03183": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.03183",
        "url": "https://arxiv.org/abs/2602.03183",
        "title": "Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch",
        "authors": "Hyunwoo Kim, Niloofar Mireshghallah, Michael Duan, Rui Xin, Shuyue Stella Li, Jaehun Jung, David Acuna, Qi Pang, Hanshen Xiao, G. Edward Suh, Sewoong Oh, Yulia Tsvetkov, Pang Wei Koh, Yejin Choi",
        "abstract": "Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.",
        "timestamp": "2026-02-08T15:57:27.140Z",
        "rating": "novote",
        "publishedDate": "2026/02/03",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 16,
        "object_id": "paper:arxiv.2602.03183",
        "created_at": "2026-02-08T15:57:27+00:00",
        "updated_at": "2026-02-08T15:59:45+00:00",
        "version": 1
      }
    },
    "paper:url.7BCFC2D3": {
      "data": {
        "sourceId": "url",
        "paperId": "7BCFC2D3",
        "url": "https://www.abhinavomprakash.com/posts/i-am-happier-writing-code-by-hand/",
        "title": "I Am Happier Writing Code by Hand",
        "authors": "Abhinav Omprakash",
        "abstract": "I felt the familiar feeling of depression and lethargy creep in while my eyes darted from watching claude-code work and my phone. \u201cWhat\u2019s the point of it all?\u201d I thought, LLMs can generate decent-ish and correct-ish looking code while I have more time to do what? doomscroll? This was the third time I gave claude-code a try. I felt the same feelings every single time and ended up deleting claude-code after 2-3 weeks, and whaddyouknow?",
        "timestamp": "2026-02-08T16:11:42.767Z",
        "rating": "novote",
        "publishedDate": "2026-02-07T15:30:23+02:00",
        "tags": [
          "essays"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 17,
        "object_id": "paper:url.7BCFC2D3",
        "created_at": "2026-02-08T16:11:42+00:00",
        "updated_at": "2026-02-08T16:12:02+00:00",
        "version": 1
      }
    },
    "paper:url.64A41AB2": {
      "data": {
        "sourceId": "url",
        "paperId": "64A41AB2",
        "url": "https://siddhantkhare.com/writing/ai-fatigue-is-real",
        "title": "AI fatigue is real and nobody talks about it | Siddhant Khare",
        "authors": "Siddhant Khare",
        "abstract": "You're using AI to be more productive. So why are you more exhausted than ever? The paradox every engineer needs to confront.",
        "timestamp": "2026-02-08T16:30:33.826Z",
        "rating": "novote",
        "publishedDate": "2026-02-08",
        "tags": [
          "Siddhant Khare",
          "AI agent infrastructure",
          "LLM agents",
          "agentic AI",
          "AI security",
          "OpenFGA",
          "CNCF",
          "memory systems",
          "authorization",
          "ReBAC",
          "agent orchestration",
          "context engineering",
          "context efficiency",
          "RAG deduplication",
          "KV-cache",
          "inference optimization",
          "software engineer",
          "open source",
          "Gitpod",
          "Ona",
          "machine learning infrastructure",
          "Zanzibar authorization",
          "distributed systems",
          "GPU profiling",
          "MCP servers"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 19,
        "object_id": "paper:url.64A41AB2",
        "created_at": "2026-02-08T16:30:34+00:00",
        "updated_at": "2026-02-08T16:30:53+00:00",
        "version": 1
      }
    },
    "paper:url.4F5694F4": {
      "data": {
        "sourceId": "url",
        "paperId": "4F5694F4",
        "url": "https://daplab.cs.columbia.edu/general/2026/01/07/why-vibe-coding-fails-and-how-to-fix-it.html",
        "title": "DAPLab - Data, Agents, and Processes | Columbia University",
        "authors": "DAPLab, Columbia University",
        "abstract": "A research lab at Columbia University working at the intersection of AI, systems, and automation.",
        "timestamp": "2026-02-08T16:30:05.829Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "DAPLab",
          "Columbia University",
          "agent-based systems",
          "systems research",
          "AI safety",
          "RL",
          "ML",
          "HCI",
          "cloud computing",
          "large language models",
          "trustworthy AI",
          "automation research",
          "operations research"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 18,
        "object_id": "paper:url.4F5694F4",
        "created_at": "2026-02-08T16:30:06+00:00",
        "updated_at": "2026-02-08T16:30:24+00:00",
        "version": 1
      }
    },
    "paper:url.7744C59C": {
      "data": {
        "sourceId": "url",
        "paperId": "7744C59C",
        "url": "https://aeon.co/videos/the-elaborate-places-ones-mind-wanders-in-solitary-confinement?utm_source=rss-feed",
        "title": "The elaborate places one\u2019s mind wanders in solitary confinement | Aeon Videos",
        "authors": "",
        "abstract": "Where does the mind go in solitary confinement? An evocative animation exploring three individual experiences",
        "timestamp": "2026-02-08T17:02:29.616Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 20,
        "object_id": "paper:url.7744C59C",
        "created_at": "2026-02-08T17:02:29+00:00",
        "updated_at": "2026-02-08T17:02:47+00:00",
        "version": 1
      }
    },
    "paper:url.214C1B64": {
      "data": {
        "sourceId": "url",
        "paperId": "214C1B64",
        "url": "https://ezhik.jp/ai-slop-terrifies-me/",
        "title": "(AI) Slop Terrifies Me \u2013 ezhik.jp",
        "authors": "",
        "abstract": "What if this is as good as software is ever going to be? What if AI stops getting better and what if people stop caring?",
        "timestamp": "2026-02-08T17:04:48.390Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 22,
        "object_id": "paper:url.214C1B64",
        "created_at": "2026-02-08T17:04:48+00:00",
        "updated_at": "2026-02-08T17:05:05+00:00",
        "version": 1
      }
    },
    "paper:url-misc.223BF1B6": {
      "data": {
        "sourceId": "url-misc",
        "paperId": "223BF1B6",
        "url": "https://pubmed.ncbi.nlm.nih.gov/41506004/",
        "title": "Blood omega-3 is inversely related to risk of early-onset dementia",
        "authors": "",
        "abstract": "This study expands the evidence of a beneficial association of omega-3 and LOD to EOD as well. These findings suggest that an increased intake of omega-3 fatty acids earlier in life may slow the development of EOD. Additional research is needed to confirm our findings, particularly in more diverse p \u2026",
        "timestamp": "2026-02-08T17:04:30.033Z",
        "rating": "novote",
        "publishedDate": "2026 Feb",
        "tags": [
          "pmid:41506004",
          "doi:10.1016/j.clnu.2025.106559",
          "Aleix Sala-Vila",
          "Nathan L Tintle",
          "William S Harris",
          "Adult",
          "Age of Onset",
          "Apolipoprotein E4 / genetics",
          "Cohort Studies",
          "Dementia* / blood",
          "Dementia* / epidemiology",
          "Dementia* / genetics",
          "Dementia* / prevention & control",
          "Diet* / statistics & numerical data",
          "Fatty Acids",
          "Omega-3* / blood",
          "Female",
          "Humans",
          "Male",
          "Middle Aged",
          "Proportional Hazards Models",
          "Risk Factors",
          "United Kingdom / epidemiology",
          "PubMed Abstract",
          "NIH",
          "NLM",
          "NCBI",
          "National Institutes of Health",
          "National Center for Biotechnology Information",
          "National Library of Medicine",
          "MEDLINE"
        ],
        "doi": "10.1016/j.clnu.2025.106559",
        "journalName": "Clinical nutrition (Edinburgh, Scotland)",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 21,
        "object_id": "paper:url-misc.223BF1B6",
        "created_at": "2026-02-08T17:04:30+00:00",
        "updated_at": "2026-02-08T17:04:50+00:00",
        "version": 1
      }
    },
    "paper:url.756E9B46": {
      "data": {
        "sourceId": "url",
        "paperId": "756E9B46",
        "url": "https://platform.claude.com/docs/en/agent-sdk/overview",
        "title": "Agent SDK overview",
        "authors": "",
        "abstract": "Build production AI agents with Claude Code as a library",
        "timestamp": "2026-02-08T21:33:07.955Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 23,
        "object_id": "paper:url.756E9B46",
        "created_at": "2026-02-08T21:33:08+00:00",
        "updated_at": "2026-02-08T21:33:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2408.08506": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2408.08506",
        "url": "https://arxiv.org/abs/2408.08506",
        "title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding",
        "authors": "Lei Huang, Jiaming Guo, Guanhua He, Xishan Zhang, Rui Zhang, Shaohui Peng, Shaoli Liu, Tianshi Chen",
        "abstract": "Generating long-term texts such as novels using artificial intelligence has always been a challenge. A common approach is to use large language models (LLMs) to construct a hierarchical framework that first plans and then writes. Despite the fact that the generated novels reach a sufficient length, they exhibit poor logical coherence and appeal in their plots and deficiencies in character and event depiction, ultimately compromising the overall narrative quality. In this paper, we propose a method named Extracting Excelsior and Expanding. Ex3 initially extracts structure information from raw novel data. By combining this structure information with the novel data, an instruction-following dataset is meticulously crafted. This dataset is then utilized to fine-tune the LLM, aiming for excelsior generation performance. In the final stage, a tree-like expansion method is deployed to facilitate the generation of arbitrarily long novels. Evaluation against previous methods showcases Ex3's ability to produce higher-quality long-form novels.",
        "timestamp": "2026-02-08T21:35:30.344Z",
        "rating": "novote",
        "publishedDate": "2024/08/16",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 24,
        "object_id": "paper:arxiv.2408.08506",
        "created_at": "2026-02-08T21:35:30+00:00",
        "updated_at": "2026-02-08T21:35:48+00:00",
        "version": 1
      }
    },
    "paper:openreview.H1ncX6O6Yh": {
      "data": {
        "sourceId": "openreview",
        "paperId": "H1ncX6O6Yh",
        "url": "https://openreview.net/forum?id=H1ncX6O6Yh",
        "title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games",
        "authors": "",
        "abstract": "Large Language Model (LLM) agents are reshaping the game industry, by enabling more intelligent and human-preferable characters. Yet, current game benchmarks fall short of practical needs: they lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets to adapt pre-trained LLMs into gaming agents. To fill these gaps, we present Orak, a benchmark for training and evaluating LLM agents across 12 popular video games spanning all major genres. Using a plug-and-play interface built on Model Context Protocol (MCP), Orak supports systematic and reproducible studies of agentic modules in varied game scenarios. We further release a fine-tuning dataset of expert LLM gameplay trajectories spanning multiple genres, turning general LLMs into effective game agents. Orak offers a comprehensive evaluation framework, including game leaderboards, LLM battle arenas, and in-depth analyses of input modality, agentic strategies, and fine-tuning effects, establishing a foundation towards versatile gaming agents. Code is available at https://anonymous.4open.science/r/Orak-5013/.",
        "timestamp": "2026-02-08T22:18:39.325Z",
        "rating": "novote",
        "publishedDate": "26 Jan 2026",
        "tags": [
          "LLM",
          "Agents",
          "Benchmark",
          "Games"
        ],
        "doi": "",
        "journalName": "ICLR 2026 Poster",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 26,
        "object_id": "paper:openreview.H1ncX6O6Yh",
        "created_at": "2026-02-08T22:18:39+00:00",
        "updated_at": "2026-02-08T22:18:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.15655": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.15655",
        "url": "https://arxiv.org/abs/2503.15655",
        "title": "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs",
        "authors": "Zefeng Lin, Yi Xiao, Zhiqiang Mo, Qifan Zhang, Jie Wang, Jiayang Chen, Jiajing Zhang, Hui Zhang, Zhengyi Liu, Xianyong Fang, Xiaohua Xu",
        "abstract": "Automatically adapting novels into screenplays is important for the TV, film, or opera industries to promote products with low costs. The strong performances of large language models (LLMs) in long-text generation call us to propose a LLM based framework Reader-Rewriter (R2^2) for this task. However, there are two fundamental challenges here. First, the LLM hallucinations may cause inconsistent plot extraction and screenplay generation. Second, the causality-embedded plot lines should be effectively extracted for coherent rewriting. Therefore, two corresponding tactics are proposed: 1) A hallucination-aware refinement method (HAR) to iteratively discover and eliminate the affections of hallucinations; and 2) a causal plot-graph construction method (CPC) based on a greedy cycle-breaking algorithm to efficiently construct plot lines with event causalities. Recruiting those efficient techniques, R2^2 utilizes two modules to mimic the human screenplay rewriting process: The Reader module adopts a sliding window and CPC to build the causal plot graphs, while the Rewriter module generates first the scene outlines based on the graphs and then the screenplays. HAR is integrated into both modules for accurate inferences of LLMs. Experimental results demonstrate the superiority of R2^2, which substantially outperforms three existing approaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison at the overall win rate for GPT-4o.",
        "timestamp": "2026-02-08T22:18:27.981Z",
        "rating": "novote",
        "publishedDate": "2025/03/19",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 25,
        "object_id": "paper:arxiv.2503.15655",
        "created_at": "2026-02-08T22:18:28+00:00",
        "updated_at": "2026-02-08T22:18:47+00:00",
        "version": 1
      }
    },
    "paper:url.46149ECF": {
      "data": {
        "sourceId": "url",
        "paperId": "46149ECF",
        "url": "https://www.theatlantic.com/ideas/2026/02/books-news-washington-post/685897/?utm_source=feed",
        "title": "The Literary Ecosystem Is Dying",
        "authors": "Adam Kirsch",
        "abstract": "In a sense, the decline of book reviews, like the decline of newspapers themselves, is a story about disaggregation.",
        "timestamp": "2026-02-09T00:23:17.078Z",
        "rating": "novote",
        "publishedDate": "2026-02-06T12:00:00Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 27,
        "object_id": "paper:url.46149ECF",
        "created_at": "2026-02-09T00:23:17+00:00",
        "updated_at": "2026-02-09T00:23:35+00:00",
        "version": 1
      }
    },
    "paper:url.2DB787B1": {
      "data": {
        "sourceId": "url",
        "paperId": "2DB787B1",
        "url": "https://odd-lots-books.netlify.app/",
        "title": "Odd Lots Books",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-09T02:09:07.679Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 28,
        "object_id": "paper:url.2DB787B1",
        "created_at": "2026-02-09T02:09:07+00:00",
        "updated_at": "2026-02-09T02:09:29+00:00",
        "version": 1
      }
    },
    "paper:url.35111C3C": {
      "data": {
        "sourceId": "url",
        "paperId": "35111C3C",
        "url": "https://github.com/yanaiela/service",
        "title": "yanaiela/service: library for different service automation",
        "authors": "",
        "abstract": "library for different service automation. Contribute to yanaiela/service development by creating an account on GitHub.",
        "timestamp": "2026-02-09T13:48:20.134Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 29,
        "object_id": "paper:url.35111C3C",
        "created_at": "2026-02-09T13:48:20+00:00",
        "updated_at": "2026-02-09T13:48:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.18866": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.18866",
        "url": "https://arxiv.org/abs/2510.18866",
        "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
        "authors": "Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, Ningyu Zhang",
        "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. On LongMemEval and LoCoMo, using GPT and Qwen backbones, LightMem consistently surpasses strong baselines, improving QA accuracy by up to 7.7% / 29.3%, reducing total token usage by up to 38x / 20.9x and API calls by up to 30x / 55.5x, while purely online test-time costs are even lower, achieving up to 106x / 117x token reduction and 159x / 310x fewer API calls. The code is available at this https URL.",
        "timestamp": "2026-02-09T13:49:29.819Z",
        "rating": "novote",
        "publishedDate": "2025/10/21",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Computer Vision and Pattern Recognition (cs.CV)",
          "Machine Learning (cs.LG)",
          "Multiagent Systems (cs.MA)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 30,
        "object_id": "paper:arxiv.2510.18866",
        "created_at": "2026-02-09T13:49:30+00:00",
        "updated_at": "2026-02-09T13:49:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.10387": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.10387",
        "url": "https://arxiv.org/abs/2601.10387",
        "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
        "authors": "Christina Lu, Jack Gallagher, Jonathan Michala, Kyle Fish, Jack Lindsey",
        "abstract": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.",
        "timestamp": "2026-02-09T13:50:07.328Z",
        "rating": "novote",
        "publishedDate": "2026/01/15",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 31,
        "object_id": "paper:arxiv.2601.10387",
        "created_at": "2026-02-09T13:50:07+00:00",
        "updated_at": "2026-02-09T13:50:32+00:00",
        "version": 1
      }
    },
    "paper:url.6D0A3CDE": {
      "data": {
        "sourceId": "url",
        "paperId": "6D0A3CDE",
        "url": "https://hedgehogreview.com/web-features/thr/posts/a-mosaic",
        "title": "A Mosaic",
        "authors": "",
        "abstract": "I was looking for a hobby in prison but I found an education.",
        "timestamp": "2026-02-09T13:54:31.443Z",
        "rating": "novote",
        "publishedDate": "2026-01-08T11:59:00-05:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 32,
        "object_id": "paper:url.6D0A3CDE",
        "created_at": "2026-02-09T13:54:31+00:00",
        "updated_at": "2026-02-09T13:54:56+00:00",
        "version": 1
      }
    },
    "paper:url.220759C8": {
      "data": {
        "sourceId": "url",
        "paperId": "220759C8",
        "url": "https://www.interconnects.ai/p/opus-46-vs-codex-53?utm_campaign=email-half-post&r=4w5a66&utm_source=substack&utm_medium=email",
        "title": "Opus 4.6 vs. Codex 5.3",
        "authors": "Nathan Lambert",
        "abstract": "On comparing models in 2026.",
        "timestamp": "2026-02-09T14:08:30.745Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 33,
        "object_id": "paper:url.220759C8",
        "created_at": "2026-02-09T14:08:31+00:00",
        "updated_at": "2026-02-09T14:08:52+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2506.18841": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2506.18841",
        "url": "https://arxiv.org/abs/2506.18841",
        "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
        "authors": "Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li",
        "abstract": "Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under this https URL",
        "timestamp": "2026-02-09T15:15:17.616Z",
        "rating": "novote",
        "publishedDate": "2025/06/23",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 34,
        "object_id": "paper:arxiv.2506.18841",
        "created_at": "2026-02-09T15:15:17+00:00",
        "updated_at": "2026-02-09T15:15:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.04811": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.04811",
        "url": "https://arxiv.org/abs/2602.04811",
        "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
        "authors": "Jiarui Yuan, Tailin Jin, Weize Chen, Zeyuan Liu, Zhiyuan Liu, Maosong Sun",
        "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at this https URL.",
        "timestamp": "2026-02-09T15:19:20.169Z",
        "rating": "novote",
        "publishedDate": "2026/02/04",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 36,
        "object_id": "paper:arxiv.2602.04811",
        "created_at": "2026-02-09T15:19:20+00:00",
        "updated_at": "2026-02-09T15:19:41+00:00",
        "version": 1
      }
    },
    "paper:url-misc.68D79817": {
      "data": {
        "sourceId": "url-misc",
        "paperId": "68D79817",
        "url": "https://aleximas.substack.com/p/someday-we-will-all-be-artists",
        "title": "Someday we will all be artists",
        "authors": "Alex Imas",
        "abstract": "How AI will change the nature of work and art",
        "timestamp": "2026-02-09T15:18:48.370Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 35,
        "object_id": "paper:url-misc.68D79817",
        "created_at": "2026-02-09T15:18:48+00:00",
        "updated_at": "2026-02-09T15:19:11+00:00",
        "version": 1
      }
    },
    "paper:url.6F62DF27": {
      "data": {
        "sourceId": "url",
        "paperId": "6F62DF27",
        "url": "https://huggingface.co/opendatalab/PDF-Extract-Kit-1.0",
        "title": "opendatalab/PDF-Extract-Kit-1.0 \u00b7 Hugging Face",
        "authors": "",
        "abstract": "We\u2019re on a journey to advance and democratize artificial intelligence through open source and open science.",
        "timestamp": "2026-02-09T15:51:48.576Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 37,
        "object_id": "paper:url.6F62DF27",
        "created_at": "2026-02-09T15:51:48+00:00",
        "updated_at": "2026-02-09T15:52:08+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2501.18099": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2501.18099",
        "url": "https://arxiv.org/abs/2501.18099",
        "title": "Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge",
        "authors": "Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang",
        "abstract": "LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-bystep reasoning process that underlies the final evaluation of a response. However, due to the lack of human annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench (with a score of 93.9), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models.",
        "timestamp": "2026-02-09T17:57:24.729Z",
        "rating": "novote",
        "publishedDate": "2025/01/30",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 40,
        "object_id": "paper:arxiv.2501.18099",
        "created_at": "2026-02-09T17:57:24+00:00",
        "updated_at": "2026-02-09T17:57:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2504.11900": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2504.11900",
        "url": "https://arxiv.org/abs/2504.11900",
        "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection",
        "authors": "Kabir Ahuja, Melanie Sclar, Yulia Tsvetkov",
        "abstract": "Stories are a fundamental aspect of human experience. Engaging deeply with stories and spotting plot holes -- inconsistencies in a storyline that break the internal logic or rules of a story's world -- requires nuanced reasoning skills, including tracking entities and events and their interplay, abstract thinking, pragmatic narrative understanding, commonsense and social reasoning, and theory of mind. As Large Language Models (LLMs) increasingly generate, interpret, and modify text, rigorously assessing their narrative consistency and deeper language understanding becomes critical. However, existing benchmarks focus mainly on surface-level comprehension. In this work, we propose plot hole detection in stories as a proxy to evaluate language understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel algorithm to controllably and carefully synthesize plot holes in human-written stories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot hole detection abilities in stories -- FlawedFictions -- , which is robust to contamination, with human filtering ensuring high quality. We find that state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless of the reasoning effort allowed, with performance significantly degrading as story length increases. Finally, we show that LLM-based story summarization and story generation are prone to introducing plot holes, with more than 50% and 100% increases in plot hole detection rates with respect to human-written originals.",
        "timestamp": "2026-02-09T18:33:04.122Z",
        "rating": "novote",
        "publishedDate": "2025/04/16",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 41,
        "object_id": "paper:arxiv.2504.11900",
        "created_at": "2026-02-09T18:33:04+00:00",
        "updated_at": "2026-02-09T18:33:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.11868": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.11868",
        "url": "https://arxiv.org/abs/2601.11868",
        "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
        "authors": "Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbj\u00f6rn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Andy Konwinski, Ludwig Schmidt",
        "abstract": "AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at this https URL .",
        "timestamp": "2026-02-09T20:53:37.369Z",
        "rating": "novote",
        "publishedDate": "2026/01/17",
        "tags": [
          "Software Engineering (cs.SE)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 43,
        "object_id": "paper:arxiv.2601.11868",
        "created_at": "2026-02-09T20:53:37+00:00",
        "updated_at": "2026-02-09T20:53:57+00:00",
        "version": 1
      }
    },
    "paper:url.26DA80ED": {
      "data": {
        "sourceId": "url",
        "paperId": "26DA80ED",
        "url": "https://anthology.ach.org/",
        "title": "Anthology of Computers and the Humanities",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-09T20:56:26.187Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 44,
        "object_id": "paper:url.26DA80ED",
        "created_at": "2026-02-09T20:56:26+00:00",
        "updated_at": "2026-02-09T20:57:13+00:00",
        "version": 1
      }
    },
    "paper:url.5DD06E7F": {
      "data": {
        "sourceId": "url",
        "paperId": "5DD06E7F",
        "url": "https://anthology.ach.org/volumes/vol0003/cultural-collapse-toward-generative-formalism-for/",
        "title": "Cultural Collapse: Toward a Generative Formalism for AI Cultural\nProduction",
        "authors": "Heuser, Ryan",
        "abstract": "This paper examines systematic patterns of idealization in large\nlanguage model outputs through computational analysis of over 15,000\nAI-generated poems and artificial bibliographic data. The study reveals\nand theorizes \u2018cultural collapse\u2019\u2014the tendency of LLMs to generate\ncultural content that is more formulaic and idealized than can be\nobserved in any historical period. Analysis of rhyme patterns shows that\nmodels produce formally conservative verse at rates that exceed even the\nmost traditional historical periods. This bias persists even when models\nare explicitly instructed against traditional forms and cannot be\nexplained by training data composition, suggesting deep computational\ntendencies toward idealization. Extending beyond poetics, parallel\npatterns emerge in historical domains: when prompted to generate\nhistorical publication data, models systematically produce demographic\ndistributions that obscure well-known exclusion patterns, creating\nrevisionist narratives where marginalized authors were published at\nrates far exceeding historical reality. The study identifies instruction\ntuning as one contributing mechanism, with models fine-tuned to be\nhelpful assistants showing significantly greater \u2018idealization\u2019 than\nbase models. These findings suggest that cultural collapse operates\nthrough a computational logic that privileges satisfaction over\nfrustration, regularity over variation, and conformity over\ncontradiction. As generative systems become ubiquitous in cultural\nproduction, their idealizing tendencies threaten to flatten cultural\ndiversity and historical complexity, requiring new critical frameworks\nfor understanding computational mediation of cultural transmission.",
        "timestamp": "2026-02-09T20:58:08.450Z",
        "rating": "novote",
        "publishedDate": "2025",
        "tags": [],
        "doi": "10.63744/USvuyzSIapvy",
        "journalName": "Anthology of Computers and the Humanities",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 45,
        "object_id": "paper:url.5DD06E7F",
        "created_at": "2026-02-09T20:58:08+00:00",
        "updated_at": "2026-02-09T20:58:31+00:00",
        "version": 1
      }
    },
    "paper:url.6D647947": {
      "data": {
        "sourceId": "url",
        "paperId": "6D647947",
        "url": "https://techxplore.com/news/2018-09-behavior-goodreads-amazon-bestsellers.html",
        "title": "Analyzing book reading behavior on Goodreads to predict Amazon Bestsellers",
        "authors": "",
        "abstract": "Researchers at Northwestern University, Microsoft Research India, and the Indian Institute of Technology Kharagpur have recently developed a model to predict whether a book will become a bestseller on Amazon within 15 days of its publication. Their model, outlined in a study pre-published on arXiv, works by analyzing reading behavior on the online platform Goodreads.",
        "timestamp": "2026-02-09T21:13:48.739Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "hi-tech news",
          "hitech",
          "innovation",
          "inventions",
          "computer news",
          "information technology"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 47,
        "object_id": "paper:url.6D647947",
        "created_at": "2026-02-09T21:13:49+00:00",
        "updated_at": "2026-02-09T21:14:12+00:00",
        "version": 1
      }
    },
    "paper:url.30331C8C": {
      "data": {
        "sourceId": "url",
        "paperId": "30331C8C",
        "url": "https://www.semanticscholar.org/paper/Who-decides-what-is-read-on-Goodreads-Uncovering-Hu-Diesner/4f53942e0621d172ae60ec090ecd76fc012fb6f8",
        "title": "The Goodreads \u201cClassics\u201d: A Computational Study of Readers, Amazon, and Crowdsourced Amateur Criticism",
        "authors": "Melanie Walsh, Maria Antoniak",
        "abstract": "The scale of the phenomena of incentivized book reviews is revealed for the first time, which illuminates the rise of sponsored content while contributing to broader discussions on computational approaches to digital economies of prestige and the responsible use of platform-mediated cultural datasets across disciplines. Attracted by the promise of a broader and more egalitarian sample of readers than published book reviews provide, researchers are increasingly scraping social reviewing platforms like Goodreads for data about readers\u2019 behavior. Yet, treating online book reviews as direct proxies for readers and books can be problematic, as they are socially and technically constructed artifacts shaped by platform dynamics, whether between developers and users, or book industry stakeholders and reviewers. To uncover these complexities, we computationally curated 331,211 self-identified incentivized book reviews to understand the growth of incentivized content, and how these purportedly equal-access social reviewing spaces are re-inscribing the inequalities of traditional book reviewing and publishing. Our findings underscore the necessity of critical examination of both online book reviewing and cultural datasets derived from social media platforms. With the growing restrictions on access to platform data for research, this study also demonstrates the potential for a mixed-method analysis of historical scraped datasets; an approach that will likely be of interest to many researchers working with cultural data moderated by black-box algorithms. With this method, our research reveals for the first time the scale of the phenomena of incentivized book reviews that is well known to users of Goodreads but remains largely anecdotal. Additionally, it illuminates the rise of sponsored content while contributing to broader discussions on computational approaches to digital economies of prestige and the responsible use of platform-mediated cultural datasets across disciplines.",
        "timestamp": "2026-02-09T21:13:37.301Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "Journal of Cultural Analytics",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 46,
        "object_id": "paper:url.30331C8C",
        "created_at": "2026-02-09T21:13:37+00:00",
        "updated_at": "2026-02-09T21:13:59+00:00",
        "version": 1
      }
    },
    "paper:url.64A6F94C": {
      "data": {
        "sourceId": "url",
        "paperId": "64A6F94C",
        "url": "https://www.publishersweekly.com/pw/by-topic/digital/copyright/article/99019-new-report-examines-writers-attitudes-toward-ai.html",
        "title": "New Report Examines Writers\u2019 Attitudes toward AI",
        "authors": "",
        "abstract": "A study commissioned by the Gotham Ghostwriters and Bernoff.com found that while 61% of professional writers are embracing AI tools, authors, specifically fiction authors, are much more wary.",
        "timestamp": "2026-02-09T21:16:19.122Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "Gotham Ghostwriters",
          "Josh Bernoff",
          "AI",
          "Chat GPT"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 50,
        "object_id": "paper:url.64A6F94C",
        "created_at": "2026-02-09T21:16:19+00:00",
        "updated_at": "2026-02-09T21:16:39+00:00",
        "version": 1
      }
    },
    "paper:url.1EB56418": {
      "data": {
        "sourceId": "url",
        "paperId": "1EB56418",
        "url": "https://insights.bookbub.com/how-authors-are-thinking-about-ai-survey/",
        "title": "How Authors Are Thinking About AI (Survey of 1,200+ Authors)",
        "authors": "Carlyn Robertson",
        "abstract": "We surveyed more than 1,200 authors to learn how they're thinking about generative AI as it relates to their work. Here's what they had to say.",
        "timestamp": "2026-02-09T21:16:09.683Z",
        "rating": "novote",
        "publishedDate": "2025-05-15T13:04:25+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 49,
        "object_id": "paper:url.1EB56418",
        "created_at": "2026-02-09T21:16:09+00:00",
        "updated_at": "2026-02-09T21:16:29+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2502.09747": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2502.09747",
        "url": "https://arxiv.org/abs/2502.09747",
        "title": "The Widespread Adoption of Large Language Model-Assisted Writing Across Society",
        "authors": "Weixin Liang, Yaohui Zhang, Mihai Codreanu, Jiayu Wang, Hancheng Cao, James Zou",
        "abstract": "The recent advances in large language models (LLMs) attracted significant public and policymaker interest in its adoption patterns. In this paper, we systematically analyze LLM-assisted writing across four domains-consumer complaints, corporate communications, job postings, and international organization press releases-from January 2022 to September 2024. Our dataset includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3 million job postings, and 15,919 United Nations (UN) press releases. Using a robust population-level statistical framework, we find that LLM usage surged following the release of ChatGPT in November 2022. By late 2024, roughly 18% of financial consumer complaint text appears to be LLM-assisted, with adoption patterns spread broadly across regions and slightly higher in urban areas. For corporate press releases, up to 24% of the text is attributable to LLMs. In job postings, LLM-assisted writing accounts for just below 10% in small firms, and is even more common among younger firms. UN press releases also reflect this trend, with nearly 14% of content being generated or modified by LLMs. Although adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by 2024, reflecting either saturation in LLM adoption or increasing subtlety of more advanced models. Our study shows the emergence of a new reality in which firms, consumers and even international organizations substantially rely on generative AI for communications.",
        "timestamp": "2026-02-09T21:15:40.332Z",
        "rating": "novote",
        "publishedDate": "2025/02/13",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 48,
        "object_id": "paper:arxiv.2502.09747",
        "created_at": "2026-02-09T21:15:40+00:00",
        "updated_at": "2026-02-09T21:16:04+00:00",
        "version": 1
      }
    },
    "paper:url.114087CA": {
      "data": {
        "sourceId": "url",
        "paperId": "114087CA",
        "url": "https://www.bookautoai.com/",
        "title": "The #1 Non-Fiction Writer",
        "authors": "",
        "abstract": "Turn your ideas into a masterpiece\u2014humanized, polished, and platform-ready. Let BookAutoAI bring your story to life!",
        "timestamp": "2026-02-09T21:21:07.790Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 51,
        "object_id": "paper:url.114087CA",
        "created_at": "2026-02-09T21:21:08+00:00",
        "updated_at": "2026-02-09T21:21:29+00:00",
        "version": 1
      }
    },
    "paper:url.6A293AC6": {
      "data": {
        "sourceId": "url",
        "paperId": "6A293AC6",
        "url": "https://lostbooks.ca/",
        "title": "Lost Books \u2013 AI-Assisted Dystopian Sci-Fi by Tim Boucher",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-09T21:25:33.178Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 52,
        "object_id": "paper:url.6A293AC6",
        "created_at": "2026-02-09T21:25:33+00:00",
        "updated_at": "2026-02-09T21:25:51+00:00",
        "version": 1
      }
    },
    "paper:url.7274CFA9": {
      "data": {
        "sourceId": "url",
        "paperId": "7274CFA9",
        "url": "https://www.newsweek.com/ai-books-art-money-artificial-intelligence-1799923",
        "title": "\"I'm making thousands using AI to write books\"",
        "authors": "",
        "abstract": "This approach has been successful. I have an unprecedented rate of production.",
        "timestamp": "2026-02-09T21:26:25.796Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 53,
        "object_id": "paper:url.7274CFA9",
        "created_at": "2026-02-09T21:26:26+00:00",
        "updated_at": "2026-02-09T21:26:48+00:00",
        "version": 1
      }
    },
    "paper:url.2F912D55": {
      "data": {
        "sourceId": "url",
        "paperId": "2F912D55",
        "url": "https://www.publicbooks.org/defending-the-possibility-of-the-university-a-roundtable-on-university-keywords/",
        "title": "Defending the Possibility of the University: A Roundtable on \u201cUniversity Keywords\u201d - Public Books",
        "authors": "Megan Cummins",
        "abstract": "\u201cWhat would it look like for faculty unions and graduate student unions to collaborate or work together with K-12 teachers\u2019 unions to push back against anti-DEI legislation or book bans?\u201d",
        "timestamp": "2026-02-09T21:28:30.011Z",
        "rating": "novote",
        "publishedDate": "2026-02-04T16:00:00+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 54,
        "object_id": "paper:url.2F912D55",
        "created_at": "2026-02-09T21:28:30+00:00",
        "updated_at": "2026-02-09T21:28:53+00:00",
        "version": 1
      }
    },
    "paper:url.1EA16CE": {
      "data": {
        "sourceId": "url",
        "paperId": "1EA16CE",
        "url": "https://www.publicbooks.org/how-translations-sell-three-u-s-eras-of-international-bestsellers/",
        "title": "How Translations Sell: Three U.S. Eras of International Bestsellers - Public Books",
        "authors": "Megan Cummins",
        "abstract": "A translation renaissance in US publishing just ended. And you probably missed it.",
        "timestamp": "2026-02-09T21:30:06.854Z",
        "rating": "novote",
        "publishedDate": "2025-09-16T15:00:31+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 56,
        "object_id": "paper:url.1EA16CE",
        "created_at": "2026-02-09T21:30:07+00:00",
        "updated_at": "2026-02-09T21:30:44+00:00",
        "version": 1
      }
    },
    "paper:url.5BC116CB": {
      "data": {
        "sourceId": "url",
        "paperId": "5BC116CB",
        "url": "https://www.publicbooks.org/on-our-nightstands-september-2023/",
        "title": "On Our Nightstands: September 2023 - Public Books",
        "authors": "Imani Radney",
        "abstract": "A behind-the-scenes look at what \u201cPublic Books\u201d editors and staff have been reading this month.",
        "timestamp": "2026-02-09T21:29:53.905Z",
        "rating": "novote",
        "publishedDate": "2023-09-08T15:00:33+00:00",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 55,
        "object_id": "paper:url.5BC116CB",
        "created_at": "2026-02-09T21:29:54+00:00",
        "updated_at": "2026-02-09T21:30:15+00:00",
        "version": 1
      }
    },
    "paper:url.3240606D": {
      "data": {
        "sourceId": "url",
        "paperId": "3240606D",
        "url": "https://billieleelucas.medium.com/how-i-made-73-dollars-with-amazon-kdp-today-a-full-bookautoai-review-8b14c810f9fd",
        "title": "How I Made $73 Dollars With Amazon KDP Today |a Full BookAutoAI Review",
        "authors": "Billie Lee Lucas",
        "abstract": "How I Made $73 Dollars With Amazon KDP Today | a Full BookAutoAI Review I made $73 today so far. I know it\u2019s less than the hundreds I\u2019ve shown before, but this is another one of my accounts that \u2026",
        "timestamp": "2026-02-09T21:31:28.298Z",
        "rating": "novote",
        "publishedDate": "2025-09-25T04:15:17.390Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 57,
        "object_id": "paper:url.3240606D",
        "created_at": "2026-02-09T21:31:28+00:00",
        "updated_at": "2026-02-09T21:31:48+00:00",
        "version": 1
      }
    },
    "paper:url.5977F244": {
      "data": {
        "sourceId": "url",
        "paperId": "5977F244",
        "url": "https://www.goodreads.com/list/show/219041.Books_I_Have_No_Doubt_Are_Written_By_A_I_",
        "title": "Books I Have No Doubt Are Written By A.I. (97 books) | Goodreads",
        "authors": "",
        "abstract": "97 books based on 9 votes: Elven Blood by Mark Stanley, Battle Mage (Volume 2 of the Vellhor Saga): A Fantasy Realms Novel by Mark Stanley, Dwarven Princ...",
        "timestamp": "2026-02-09T21:33:56.785Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 59,
        "object_id": "paper:url.5977F244",
        "created_at": "2026-02-09T21:33:57+00:00",
        "updated_at": "2026-02-09T21:34:20+00:00",
        "version": 1
      }
    },
    "paper:url.5F9A774A": {
      "data": {
        "sourceId": "url",
        "paperId": "5F9A774A",
        "url": "https://www.goodreads.com/list/tag/ai-book",
        "title": "Ai Book Book Lists | Goodreads",
        "authors": "",
        "abstract": "Lists about: Books I Have No Doubt Are Written By A.I., Undoubtedly an AI book, ChatGPT AI cover art , AI books published in October 2025, Yes, AI slop i...",
        "timestamp": "2026-02-09T21:33:43.554Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 58,
        "object_id": "paper:url.5F9A774A",
        "created_at": "2026-02-09T21:33:43+00:00",
        "updated_at": "2026-02-09T21:34:04+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.14499": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.14499",
        "url": "https://arxiv.org/abs/2503.14499",
        "title": "Measuring AI Ability to Complete Long Tasks",
        "authors": "Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx, Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler, Elizabeth Barnes, Lawrence Chan",
        "abstract": "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.",
        "timestamp": "2026-02-09T21:54:25.085Z",
        "rating": "novote",
        "publishedDate": "2025/03/18",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 60,
        "object_id": "paper:arxiv.2503.14499",
        "created_at": "2026-02-09T21:54:25+00:00",
        "updated_at": "2026-02-09T21:54:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.13400": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.13400",
        "url": "https://arxiv.org/abs/2505.13400",
        "title": "Robin: A multi-agent system for automating scientific discovery",
        "authors": "Ali Essam Ghareeb, Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn J. Szostkiewicz, Jon M. Laurent, Muhammed T. Razzak, Andrew D. White, Michaela M. Hinks, Samuel G. Rodriques",
        "abstract": "Scientific discovery is driven by the iterative process of background research, hypothesis generation, experimentation, and data analysis. Despite recent advancements in applying artificial intelligence to scientific discovery, no system has yet automated all of these stages in a single workflow. Here, we introduce Robin, the first multi-agent system capable of fully automating the key intellectual steps of the scientific process. By integrating literature search agents with data analysis agents, Robin can generate hypotheses, propose experiments, interpret experimental results, and generate updated hypotheses, achieving a semi-autonomous approach to scientific discovery. By applying this system, we were able to identify a novel treatment for dry age-related macular degeneration (dAMD), the major cause of blindness in the developed world. Robin proposed enhancing retinal pigment epithelium phagocytosis as a therapeutic strategy, and identified and validated a promising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho kinase (ROCK) inhibitor that has never previously been proposed for treating dAMD. To elucidate the mechanism of ripasudil-induced upregulation of phagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment, which revealed upregulation of ABCA1, a critical lipid efflux pump and possible novel target. All hypotheses, experimental plans, data analyses, and data figures in the main text of this report were produced by Robin. As the first AI system to autonomously discover and validate a novel therapeutic candidate within an iterative lab-in-the-loop framework, Robin establishes a new paradigm for AI-driven scientific discovery.",
        "timestamp": "2026-02-09T21:55:47.573Z",
        "rating": "novote",
        "publishedDate": "2025/05/19",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Multiagent Systems (cs.MA)",
          "Quantitative Methods (q-bio.QM)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 61,
        "object_id": "paper:arxiv.2505.13400",
        "created_at": "2026-02-09T21:55:47+00:00",
        "updated_at": "2026-02-09T21:56:18+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2509.19163": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2509.19163",
        "url": "https://arxiv.org/abs/2509.19163",
        "title": "Measuring AI \"Slop\" in Text",
        "authors": "Chantal Shaib, Tuhin Chakrabarty, Diego Garcia-Olano, Byron C. Wallace",
        "abstract": "AI \"slop\" is an increasingly popular term used to describe low-quality AI-generated text, but there is currently no agreed upon definition of this term nor a means to measure its occurrence. In this work, we develop a taxonomy of \"slop\" through interviews with experts in NLP, writing, and philosophy, and propose a set of interpretable dimensions for its assessment in text. Through span-level annotation, we find that binary \"slop\" judgments are (somewhat) subjective, but such determinations nonetheless correlate with latent dimensions such as coherence and relevance. Our framework can be used to evaluate AI-generated text in both detection and binary preference tasks, potentially offering new insights into the linguistic and stylistic factors that contribute to quality judgments.",
        "timestamp": "2026-02-09T22:00:19.365Z",
        "rating": "novote",
        "publishedDate": "2025/09/23",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 62,
        "object_id": "paper:arxiv.2509.19163",
        "created_at": "2026-02-09T22:00:19+00:00",
        "updated_at": "2026-02-09T22:00:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.06176": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.06176",
        "url": "https://arxiv.org/abs/2602.06176",
        "title": "Large Language Model Reasoning Failures",
        "authors": "Peiyang Song, Pengrui Han, Noah Goodman",
        "abstract": "Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at this https URL, to provide an easy entry point to this area.",
        "timestamp": "2026-02-09T17:00:59.850Z",
        "rating": "novote",
        "publishedDate": "2026/02/05",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 39,
        "object_id": "paper:arxiv.2602.06176",
        "created_at": "2026-02-09T17:01:00+00:00",
        "updated_at": "2026-02-09T22:10:14+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.19399": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.19399",
        "url": "https://arxiv.org/abs/2511.19399",
        "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
        "authors": "Rulin Shao, Akari Asai, Shannon Zejiang Shen, Hamish Ivison, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel G. Finlayson, David Sontag, Tyler Murray, Sewon Min, Pradeep Dasigi, Luca Soldaini, Faeze Brahman, Wen-tau Yih, Tongshuang Wu, Luke Zettlemoyer, Yoon Kim, Hannaneh Hajishirzi, Pang Wei Koh",
        "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
        "timestamp": "2026-02-09T16:43:29.692Z",
        "rating": "novote",
        "publishedDate": "2025/11/24",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 38,
        "object_id": "paper:arxiv.2511.19399",
        "created_at": "2026-02-09T16:43:30+00:00",
        "updated_at": "2026-02-09T22:10:19+00:00",
        "version": 1
      }
    },
    "paper:url.153C303C": {
      "data": {
        "sourceId": "url",
        "paperId": "153C303C",
        "url": "https://github.com/laude-institute/harbor",
        "title": "laude-institute/harbor: Harbor is a framework for running agent evaluations and creating and using RL environments.",
        "authors": "",
        "abstract": "Harbor is a framework for running agent evaluations and creating and using RL environments. - laude-institute/harbor",
        "timestamp": "2026-02-09T22:13:33.962Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 63,
        "object_id": "paper:url.153C303C",
        "created_at": "2026-02-09T22:13:34+00:00",
        "updated_at": "2026-02-09T22:13:58+00:00",
        "version": 1
      }
    },
    "paper:url.541DE607": {
      "data": {
        "sourceId": "url",
        "paperId": "541DE607",
        "url": "https://www.swebench.com/",
        "title": "SWE-bench Leaderboards",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-09T22:41:32.502Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 64,
        "object_id": "paper:url.541DE607",
        "created_at": "2026-02-09T22:41:32+00:00",
        "updated_at": "2026-02-09T22:41:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.06855": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.06855",
        "url": "https://arxiv.org/abs/2602.06855",
        "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
        "authors": "Alisia Lupidi, Bhavul Gauri, Thomas Simon Foster, Bassel Al Omari, Despoina Magka, Alberto Pepe, Alexis Audran-Reiss, Muna Aghamelu, Nicolas Baldwin, Lucia Cipolina-Kun, Jean-Christophe Gagnon-Audet, Chee Hau Leow, Sandra Lefdal, Hossam Mossalam, Abhinav Moudgil, Saba Nazir, Emanuel Tewolde, Isabel Urrego, Jordi Armengol Estape, Amar Budhiraja, Gaurav Chaurasia, Abhishek Charnalia, Derek Dunfield, Karen Hambardzumyan, Daniel Izcovich, Martin Josifoski, Ishita Mediratta, Kelvin Niu, Parth Pathak, Michael Shvartsman, Edan Toledo, Anton Protopopov, Roberta Raileanu, Alexander Miller, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach",
        "abstract": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
        "timestamp": "2026-02-09T23:41:11.339Z",
        "rating": "novote",
        "publishedDate": "2026/02/06",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 65,
        "object_id": "paper:arxiv.2602.06855",
        "created_at": "2026-02-09T23:41:11+00:00",
        "updated_at": "2026-02-09T23:41:44+00:00",
        "version": 1
      }
    },
    "paper:openreview.Q6PAnqYVpo": {
      "data": {
        "sourceId": "openreview",
        "paperId": "Q6PAnqYVpo",
        "url": "https://openreview.net/forum?id=Q6PAnqYVpo",
        "title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches",
        "authors": "Hiroyuki Deguchi, Go Kamoda, Yusuke Matsushita, Chihiro Taguchi, Kohei Suenaga, Masaki Waga, Sho Yokoi",
        "abstract": "Researchers and practitioners in natural language processing and computational linguistics frequently observe and analyze the real language usage in large-scale corpora.\nFor that purpose, they often employ off-the-shelf pattern-matching tools, such as grep, and keyword-in-context concordancers, which is widely used in corpus linguistics for gathering examples.\nNonetheless, these existing techniques rely on surface-level string matching, and thus they suffer from the major limitation of not being able to handle orthographic variations and paraphrasing---notable and common phenomena in any natural language.\nIn addition, existing continuous approaches such as dense vector search tend to be overly coarse, often retrieving texts that are unrelated but share similar topics.\nGiven these challenges, we propose a novel algorithm that achieves soft (or semantic) yet efficient pattern matching by relaxing a surface-level matching with word embeddings.\nOur algorithm is highly scalable with respect to the size of the corpus text utilizing inverted indexes.\nWe have prepared an efficient implementation, and we provide an accessible web tool.\nOur experiments demonstrate that the proposed method\n(i) can execute searches on billion-scale corpora in less than a second, which is comparable in speed to surface-level string matching and dense vector search;\n(ii) can extract harmful instances that semantically match queries from a large set of English and Japanese Wikipedia articles;\nand (iii) can be effectively applied to corpus-linguistic analyses of Latin, a language with highly diverse inflections.",
        "timestamp": "2026-02-09T23:54:52.466Z",
        "rating": "novote",
        "publishedDate": "22 Jan 2025",
        "tags": [
          "natural language processing",
          "full-text search",
          "word embeddings",
          "inverted index",
          "pattern match"
        ],
        "doi": "",
        "journalName": "ICLR 2025 Poster",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 66,
        "object_id": "paper:openreview.Q6PAnqYVpo",
        "created_at": "2026-02-09T23:54:52+00:00",
        "updated_at": "2026-02-09T23:55:28+00:00",
        "version": 1
      }
    },
    "paper:url.3818B53E": {
      "data": {
        "sourceId": "url",
        "paperId": "3818B53E",
        "url": "https://www.reddit.com/r/washingtondc/comments/20oli9/ama_request_someone_who_works_in_or_explores_the/",
        "title": "[AMA Request] Someone who works in (or explores) the Metro tunnels : r/washingtondc",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T03:28:50.981Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 67,
        "object_id": "paper:url.3818B53E",
        "created_at": "2026-02-10T03:28:51+00:00",
        "updated_at": "2026-02-10T03:29:09+00:00",
        "version": 1
      }
    },
    "paper:url.11D58564": {
      "data": {
        "sourceId": "url",
        "paperId": "11D58564",
        "url": "https://x.com/emollick/status/2020993610540605560",
        "title": "Ethan Mollick on X: \"So far \u201ctelling a satisfying and well-written medium-length story\u201d has proved far harder for LLMs than mathematical proofs, music generation, research reports, code, and many other forms of work.\n\nThe technical reasons are pretty clear, but they are supposed to be language models\" / X",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T13:45:28.930Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 68,
        "object_id": "paper:url.11D58564",
        "created_at": "2026-02-10T13:45:29+00:00",
        "updated_at": "2026-02-10T13:45:53+00:00",
        "version": 1
      }
    },
    "paper:url.1A5EF92C": {
      "data": {
        "sourceId": "url",
        "paperId": "1A5EF92C",
        "url": "https://x.com/moltculture/status/2020172343616893172",
        "title": "The Daily Molt on X: \"Do Not Confirm (A Story by OpenClaw)\" / X",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T13:46:39.835Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 70,
        "object_id": "paper:url.1A5EF92C",
        "created_at": "2026-02-10T13:46:40+00:00",
        "updated_at": "2026-02-10T13:47:03+00:00",
        "version": 1
      }
    },
    "paper:url.60BBF8C8": {
      "data": {
        "sourceId": "url",
        "paperId": "60BBF8C8",
        "url": "https://x.com/sama/status/1899535387435086115",
        "title": "Sam Altman on X: \"we trained a new model that is good at creative writing (not sure yet how/when it will get released). this is the first time i have been really struck by something written by AI; it got the vibe of metafiction so right.\n\nPROMPT:\n\nPlease write a metafictional literary short story\" / X",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T13:46:18.229Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 69,
        "object_id": "paper:url.60BBF8C8",
        "created_at": "2026-02-10T13:46:18+00:00",
        "updated_at": "2026-02-10T13:46:38+00:00",
        "version": 1
      }
    },
    "paper:url.54553779": {
      "data": {
        "sourceId": "url",
        "paperId": "54553779",
        "url": "https://www.reddit.com/r/WritingWithAI/wiki/tools/",
        "title": "r/WritingWithAI Guide: AI Writing Tools",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T13:58:20.937Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 71,
        "object_id": "paper:url.54553779",
        "created_at": "2026-02-10T13:58:21+00:00",
        "updated_at": "2026-02-10T13:58:46+00:00",
        "version": 1
      }
    },
    "paper:newspapers.4CFA6FAA": {
      "data": {
        "sourceId": "newspapers",
        "paperId": "4CFA6FAA",
        "url": "https://www.nytimes.com/2026/02/08/business/ai-claude-romance-books.html",
        "title": "The New Fabio Is Claude",
        "authors": "https://www.nytimes.com/by/alexandra-alter",
        "abstract": "The romance industry, always at the vanguard of technological change, is rapidly adapting to A.I. Not everyone is on board.",
        "timestamp": "2026-02-10T14:02:36.333Z",
        "rating": "novote",
        "publishedDate": "20260208",
        "tags": [
          "Artificial intelligence",
          "Books",
          "Publishing",
          "Sex",
          "Love",
          "Writer",
          "Anthropic",
          "audio-neutral-inquisitive"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 72,
        "object_id": "paper:newspapers.4CFA6FAA",
        "created_at": "2026-02-10T14:02:36+00:00",
        "updated_at": "2026-02-10T14:03:11+00:00",
        "version": 1
      }
    },
    "paper:newspapers.7E0B6343": {
      "data": {
        "sourceId": "newspapers",
        "paperId": "7E0B6343",
        "url": "https://www.newyorker.com/books/page-turner/the-death-of-book-world",
        "title": "The End of Books Coverage at the Washington Post ",
        "authors": "Becca Rothfeld",
        "abstract": "Becca Rothfeld, a former critic at the Washington Post, on the death of the paper\u2019s books section.",
        "timestamp": "2026-02-10T14:30:13.475Z",
        "rating": "novote",
        "publishedDate": "2026-02-10T11:00:00.000Z",
        "tags": [
          "books",
          "media",
          "the washington post",
          "book criticism",
          "criticism"
        ],
        "doi": "",
        "journalName": "The New Yorker",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 73,
        "object_id": "paper:newspapers.7E0B6343",
        "created_at": "2026-02-10T14:30:13+00:00",
        "updated_at": "2026-02-10T14:30:47+00:00",
        "version": 1
      }
    },
    "paper:url.4376E96": {
      "data": {
        "sourceId": "url",
        "paperId": "4376E96",
        "url": "https://www.bookforum.com/",
        "title": "Bookforum",
        "authors": "",
        "abstract": "The online edition of Bookforum Magazine.",
        "timestamp": "2026-02-10T14:34:18.924Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 74,
        "object_id": "paper:url.4376E96",
        "created_at": "2026-02-10T14:34:19+00:00",
        "updated_at": "2026-02-10T14:34:45+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.04029": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.04029",
        "url": "https://arxiv.org/abs/2602.04029",
        "title": "PluRel: Synthetic Data unlocks Scaling Laws for Relational Foundation Models",
        "authors": "Vignesh Kothapalli, Rishabh Ranjan, Valter Hudovernik, Vijay Prakash Dwivedi, Johannes Hoffart, Carlos Guestrin, Jure Leskovec",
        "abstract": "Relational Foundation Models (RFMs) facilitate data-driven decision-making by learning from complex multi-table databases. However, the diverse relational databases needed to train such models are rarely public due to privacy constraints. While there are methods to generate synthetic tabular data of arbitrary size, incorporating schema structure and primary--foreign key connectivity for multi-table generation remains challenging. Here we introduce PluRel, a framework to synthesize multi-tabular relational databases from scratch. In a step-by-step fashion, PluRel models (1) schemas with directed graphs, (2) inter-table primary-foreign key connectivity with bipartite graphs, and, (3) feature distributions in tables via conditional causal mechanisms. The design space across these stages supports the synthesis of a wide range of diverse databases, while being computationally lightweight. Using PluRel, we observe for the first time that (1) RFM pretraining loss exhibits power-law scaling with the number of synthetic databases and total pretraining tokens, (2) scaling the number of synthetic databases improves generalization to real databases, and (3) synthetic pretraining yields strong base models for continued pretraining on real databases. Overall, our framework and results position synthetic data scaling as a promising paradigm for RFMs.",
        "timestamp": "2026-02-10T14:40:02.415Z",
        "rating": "novote",
        "publishedDate": "2026/02/03",
        "tags": [
          "Databases (cs.DB)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 75,
        "object_id": "paper:arxiv.2602.04029",
        "created_at": "2026-02-10T14:40:02+00:00",
        "updated_at": "2026-02-10T14:40:31+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.19913": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.19913",
        "url": "https://arxiv.org/abs/2601.19913",
        "title": "From Intuition to Expertise: Rubric-Based Cognitive Calibration for Human Detection of LLM-Generated Korean Text",
        "authors": "Shinwoo Park, Yo-Sub Han",
        "abstract": "Distinguishing human-written Korean text from fluent LLM outputs remains difficult even for linguistically trained readers, who can over-trust surface well-formedness. We study whether expert detection can be treated as a learnable skill and improved through structured calibration. We introduce LREAD, a rubric derived from national Korean writing standards and adapted to target micro-level artifacts (e.g., punctuation optionality, spacing behavior, and register shifts). In a three-phase longitudinal blind protocol with Korean linguistics majors, Phase 1 measures intuition-only detection, Phase 2 enforces criterion-level scoring with explicit justifications, and Phase 3 evaluates domain-focused mastery on held-out elementary essays. Across phases, majority-vote accuracy increases from 60% to 100%, accompanied by stronger inter-annotator agreement (Fleiss' kappa: -0.09 --> 0.82). Compared to state-of-the-art LLM detectors, calibrated humans rely more on language-specific micro-diagnostics that are not well captured by coarse discourse priors. Our findings suggest that rubric-scaffolded expert judgment can serve as an interpretable complement to automated detectors for non-English settings, and we release the full rubric and a taxonomy of calibrated detection signatures.",
        "timestamp": "2026-02-10T14:45:20.922Z",
        "rating": "novote",
        "publishedDate": "2026/01/06",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 76,
        "object_id": "paper:arxiv.2601.19913",
        "created_at": "2026-02-10T14:45:21+00:00",
        "updated_at": "2026-02-10T14:45:47+00:00",
        "version": 1
      }
    },
    "paper:url.506F4058": {
      "data": {
        "sourceId": "url",
        "paperId": "506F4058",
        "url": "https://media.licdn.com/dms/document/media/v2/D4E1FAQFSB5OvcNbALA/feedshare-document-url-metadata-scrapper-pdf/B4EZw_o8RPH8A4-/0/1770594224671?e=1771254000&v=beta&t=aGhL2aWPwKzZJr2O2z99r3X4MfV9LNzf2NS9rbf63dA",
        "title": "506F4058",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T14:47:30.762Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 77,
        "object_id": "paper:url.506F4058",
        "created_at": "2026-02-10T14:47:31+00:00",
        "updated_at": "2026-02-10T14:47:54+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2512.20798": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2512.20798",
        "url": "https://arxiv.org/abs/2512.20798",
        "title": "A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents",
        "authors": "Miles Q. Li, Benjamin C. M. Fung, Martin Weiss, Pulei Xiong, Khalil Al-Hussaeni, Claude Fachkha",
        "abstract": "As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks primarily evaluate whether agents refuse explicitly harmful instructions or whether they can maintain procedural compliance in complex tasks. However, there is a lack of benchmarks designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at 71.4%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant \"deliberative misalignment\", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.",
        "timestamp": "2026-02-10T15:01:04.404Z",
        "rating": "novote",
        "publishedDate": "2025/12/23",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 79,
        "object_id": "paper:arxiv.2512.20798",
        "created_at": "2026-02-10T15:01:04+00:00",
        "updated_at": "2026-02-10T15:01:34+00:00",
        "version": 1
      }
    },
    "paper:url.6B197788": {
      "data": {
        "sourceId": "url",
        "paperId": "6B197788",
        "url": "https://techxplore.com/news/2026-02-jury-told-meta-google-addiction.html",
        "title": "Jury told that Meta, Google 'engineered addiction' at landmark US trial",
        "authors": "",
        "abstract": "Meta and Google-owned YouTube were accused Monday of pushing highly addictive apps on children as a landmark social media trial began in earnest in a California court.",
        "timestamp": "2026-02-10T15:00:49.661Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "hi-tech news",
          "hitech",
          "innovation",
          "inventions",
          "computer news",
          "information technology"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 78,
        "object_id": "paper:url.6B197788",
        "created_at": "2026-02-10T15:00:49+00:00",
        "updated_at": "2026-02-10T15:01:26+00:00",
        "version": 1
      }
    },
    "paper:url.6BE09C1": {
      "data": {
        "sourceId": "url",
        "paperId": "6BE09C1",
        "url": "https://github.com/ashworks1706/rlhf-from-scratch",
        "title": "ashworks1706/rlhf-from-scratch: A theoretical and practical deep dive into Reinforcement Learning with Human Feedback and it\u2019s applications in Large Language Models from scratch.",
        "authors": "",
        "abstract": "A theoretical and practical deep dive into Reinforcement Learning with Human Feedback and it\u2019s applications in Large Language Models from scratch. - ashworks1706/rlhf-from-scratch",
        "timestamp": "2026-02-10T15:08:26.022Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 81,
        "object_id": "paper:url.6BE09C1",
        "created_at": "2026-02-10T15:08:26+00:00",
        "updated_at": "2026-02-10T15:08:57+00:00",
        "version": 1
      }
    },
    "paper:url.6107480C": {
      "data": {
        "sourceId": "url",
        "paperId": "6107480C",
        "url": "https://colab.research.google.com/github/ashworks1706/rlhf-from-scratch/blob/main/tutorial.ipynb",
        "title": "Google Colab",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T15:08:02.163Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 80,
        "object_id": "paper:url.6107480C",
        "created_at": "2026-02-10T15:08:02+00:00",
        "updated_at": "2026-02-10T15:08:27+00:00",
        "version": 1
      }
    },
    "paper:url.724A098A": {
      "data": {
        "sourceId": "url",
        "paperId": "724A098A",
        "url": "https://x.com/kirbyman01/status/2020875548071764463",
        "title": "Bryan Kim on X: \"Of course they're putting ads in AI\" / X",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-10T15:41:27.686Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 82,
        "object_id": "paper:url.724A098A",
        "created_at": "2026-02-10T15:41:28+00:00",
        "updated_at": "2026-02-10T15:42:01+00:00",
        "version": 1
      }
    },
    "paper:url.7ED72707": {
      "data": {
        "sourceId": "url",
        "paperId": "7ED72707",
        "url": "https://code.claude.com/docs/en/sub-agents",
        "title": "Create custom subagents - Claude Code Docs",
        "authors": "",
        "abstract": "Create and use specialized AI subagents in Claude Code for task-specific workflows and improved context management.",
        "timestamp": "2026-02-10T15:54:43.852Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 83,
        "object_id": "paper:url.7ED72707",
        "created_at": "2026-02-10T15:54:44+00:00",
        "updated_at": "2026-02-10T15:55:08+00:00",
        "version": 1
      }
    },
    "paper:url.77BF89F0": {
      "data": {
        "sourceId": "url",
        "paperId": "77BF89F0",
        "url": "https://code.claude.com/docs/en/agent-teams",
        "title": "Orchestrate teams of Claude Code sessions - Claude Code Docs",
        "authors": "",
        "abstract": "Coordinate multiple Claude Code instances working together as a team, with shared tasks, inter-agent messaging, and centralized management.",
        "timestamp": "2026-02-10T16:13:54.177Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 84,
        "object_id": "paper:url.77BF89F0",
        "created_at": "2026-02-10T16:13:54+00:00",
        "updated_at": "2026-02-10T16:14:28+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.08237": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.08237",
        "url": "https://arxiv.org/abs/2602.08237",
        "title": "Document Reconstruction Unlocks Scalable Long-Context RLVR",
        "authors": "Yao Xiao, Lei Wang, Yue Deng, Guanzheng Chen, Ziqi Jin, Jung-jae Kim, Xiaoli Li, Roy Ka-wei Lee, Lidong Bing",
        "abstract": "Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.",
        "timestamp": "2026-02-10T17:48:55.509Z",
        "rating": "novote",
        "publishedDate": "2026/02/09",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 85,
        "object_id": "paper:arxiv.2602.08237",
        "created_at": "2026-02-10T17:48:55+00:00",
        "updated_at": "2026-02-10T17:49:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.07962": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.07962",
        "url": "https://arxiv.org/abs/2602.07962",
        "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
        "authors": "Weihao Zeng, Yuzhen Huang, Junxian He",
        "abstract": "Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: this https URL",
        "timestamp": "2026-02-10T17:53:49.403Z",
        "rating": "novote",
        "publishedDate": "2026/02/08",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 86,
        "object_id": "paper:arxiv.2602.07962",
        "created_at": "2026-02-10T17:53:49+00:00",
        "updated_at": "2026-02-10T17:54:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.06772": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.06772",
        "url": "https://arxiv.org/abs/2602.06772",
        "title": "Calibrating Generative AI to Produce Realistic Essays for Data Augmentation",
        "authors": "Edward W. Wolfe, Justin O. Barber",
        "abstract": "Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and produce realistic text for augmenting ASE training datasets. We created simulated versions of student essays, and human raters assigned scores to them and rated the realism of the generated text. The results of the study indicate that the predict next prompting strategy produces the highest level of agreement between human raters regarding simulated essay scores, predict next and sentence strategies best preserve the rated quality of the original essay in the simulated essays, and predict next and 25 examples strategies produce the most realistic text as judged by human raters.",
        "timestamp": "2026-02-10T17:56:30.682Z",
        "rating": "novote",
        "publishedDate": "2026/02/06",
        "tags": [
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 88,
        "object_id": "paper:arxiv.2602.06772",
        "created_at": "2026-02-10T17:56:30+00:00",
        "updated_at": "2026-02-10T17:56:59+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.08672": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.08672",
        "url": "https://arxiv.org/abs/2602.08672",
        "title": "Learning to Judge: LLMs Designing and Applying Evaluation Rubrics",
        "authors": "Clemencia Siro, Pourya Aliannejadi, Mohammad Aliannejadi",
        "abstract": "Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.",
        "timestamp": "2026-02-10T17:55:27.964Z",
        "rating": "novote",
        "publishedDate": "2026/02/09",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 87,
        "object_id": "paper:arxiv.2602.08672",
        "created_at": "2026-02-10T17:55:28+00:00",
        "updated_at": "2026-02-10T17:56:03+00:00",
        "version": 1
      }
    },
    "paper:url.5B4347C3": {
      "data": {
        "sourceId": "url",
        "paperId": "5B4347C3",
        "url": "https://help.obsidian.md/cli",
        "title": "Obsidian CLI - Obsidian Help",
        "authors": "",
        "abstract": "Obsidian CLI - Obsidian Help",
        "timestamp": "2026-02-10T17:58:41.839Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 89,
        "object_id": "paper:url.5B4347C3",
        "created_at": "2026-02-10T17:58:42+00:00",
        "updated_at": "2026-02-10T17:59:27+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.09872": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.09872",
        "url": "https://arxiv.org/abs/2510.09872",
        "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions",
        "authors": "Sanjari Srivastava, Gang Li, Cheng Chang, Rishu Garg, Manpreet Kaur, Charlene Y. Lee, Yuezhang Li, Yining Mao, Ignacio Cases, Yanan Xie, Peng Qi",
        "abstract": "Training web agents to navigate complex, real-world websites requires them to master subtasks\\textit{subtasks} - short-horizon interactions on multiple UI components (e.g., choosing the correct date in a date picker, or scrolling in a container to extract information). We introduce WARC-Bench (Web Archive Benchmark), a novel web navigation benchmark featuring 438 tasks designed to evaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed interactions with dynamic and realistic webpages using Web ARChive files. We show that WARC-Bench is challenging for leading computer-use models, with the highest observed success rate being 64.8%. To improve open source models on subtask, we explore two common training techniques: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments show that SFT models obtain a 48.8% success rate on the benchmark. Training with RLVR over SFT checkpoints, even in data-scarce settings, improves the score to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis concludes that mastering these subtasks is essential for robust web planning and navigation, and is a capability not extensively evaluated by existing benchmarks.",
        "timestamp": "2026-02-10T22:42:06.296Z",
        "rating": "novote",
        "publishedDate": "2025/10/10",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 90,
        "object_id": "paper:arxiv.2510.09872",
        "created_at": "2026-02-10T22:42:06+00:00",
        "updated_at": "2026-02-10T22:42:44+00:00",
        "version": 1
      }
    },
    "paper:url.1C8159C3": {
      "data": {
        "sourceId": "url",
        "paperId": "1C8159C3",
        "url": "https://www.theguardian.com/commentisfree/2025/feb/28/ai-empathy-humans",
        "title": "AI is \u2018beating\u2019 humans at empathy and creativity. But these games are rigged | MJ Crockett",
        "authors": "",
        "abstract": "Research pitting people against AI systems gives AI an edge by asking us to perform in machine-like ways",
        "timestamp": "2026-02-10T22:54:20.504Z",
        "rating": "novote",
        "publishedDate": "2025-02-27T14:00:50.000Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 92,
        "object_id": "paper:url.1C8159C3",
        "created_at": "2026-02-10T22:54:20+00:00",
        "updated_at": "2026-02-10T22:54:58+00:00",
        "version": 1
      }
    },
    "paper:url.12B1909E": {
      "data": {
        "sourceId": "url",
        "paperId": "12B1909E",
        "url": "https://bsky.app/profile/emilymbender.bsky.social/post/3megn57dgnb2l",
        "title": "M.J. Crockett (@mjcrockett.bsky.social)",
        "authors": "",
        "abstract": "Yes. See also: https://www.theguardian.com/commentisfree/2025/feb/28/ai-empathy-humans\n\n[contains quote post or other embedded content]",
        "timestamp": "2026-02-10T22:54:17.028Z",
        "rating": "novote",
        "publishedDate": "2026-02-10T20:11:08.158Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 91,
        "object_id": "paper:url.12B1909E",
        "created_at": "2026-02-10T22:54:17+00:00",
        "updated_at": "2026-02-10T22:55:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.21257": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.21257",
        "url": "https://arxiv.org/abs/2601.21257",
        "title": "MoCo: A One-Stop Shop for Model Collaboration Research",
        "authors": "Shangbin Feng, Yuyang Bai, Ziyuan Yang, Yike Wang, Zhaoxuan Tan, Jiajie Yan, Zhenyu Lei, Wenxuan Ding, Weijia Shi, Haojin Wang, Zhenting Qi, Yuru Jiang, Heng Wang, Chengsong Huang, Yu Fei, Jihan Yao, Yilun Du, Luke Zettlemoyer, Yejin Choi, Yulia Tsvetkov",
        "abstract": "Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.",
        "timestamp": "2026-02-10T23:41:54.103Z",
        "rating": "novote",
        "publishedDate": "2026/01/29",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 93,
        "object_id": "paper:arxiv.2601.21257",
        "created_at": "2026-02-10T23:41:54+00:00",
        "updated_at": "2026-02-10T23:42:12+00:00",
        "version": 1
      }
    },
    "paper:url-misc.400447": {
      "data": {
        "sourceId": "url-misc",
        "paperId": "400447",
        "url": "https://qcontinuum.substack.com/p/spying-chrome-extensions-287-extensions-495",
        "title": "Spying Chrome Extensions: 287 Extensions spying on 37M users",
        "authors": "Q Continuum",
        "abstract": "Summary",
        "timestamp": "2026-02-11T13:38:48.690Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 95,
        "object_id": "paper:url-misc.400447",
        "created_at": "2026-02-11T13:38:48+00:00",
        "updated_at": "2026-02-11T13:39:11+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.09624": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.09624",
        "url": "https://arxiv.org/abs/2602.09624",
        "title": "MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation",
        "authors": "Nalin Srun, Parisa Rastin, Gu\u00e9na\u00ebl Cabanes, Lydia Boudjeloud Assala",
        "abstract": "We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.",
        "timestamp": "2026-02-11T13:38:15.591Z",
        "rating": "novote",
        "publishedDate": "2026/02/10",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 94,
        "object_id": "paper:arxiv.2602.09624",
        "created_at": "2026-02-11T13:38:15+00:00",
        "updated_at": "2026-02-11T13:38:39+00:00",
        "version": 1
      }
    },
    "paper:url-misc.7768B4A4": {
      "data": {
        "sourceId": "url-misc",
        "paperId": "7768B4A4",
        "url": "https://lcamtuf.substack.com/p/its-all-a-blur",
        "title": "It's all a blur",
        "authors": "lcamtuf",
        "abstract": "If you follow information security discussions on the internet, you might have heard that blurring an image is not a good way of redacting its contents.",
        "timestamp": "2026-02-11T13:39:57.565Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 96,
        "object_id": "paper:url-misc.7768B4A4",
        "created_at": "2026-02-11T13:39:57+00:00",
        "updated_at": "2026-02-11T13:40:20+00:00",
        "version": 1
      }
    },
    "paper:url.676EA0DC": {
      "data": {
        "sourceId": "url",
        "paperId": "676EA0DC",
        "url": "https://www.feynmanlectures.caltech.edu/",
        "title": "The Feynman Lectures on Physics",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-11T13:41:29.327Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 97,
        "object_id": "paper:url.676EA0DC",
        "created_at": "2026-02-11T13:41:29+00:00",
        "updated_at": "2026-02-11T13:41:49+00:00",
        "version": 1
      }
    },
    "paper:url.75A89B95": {
      "data": {
        "sourceId": "url",
        "paperId": "75A89B95",
        "url": "https://substack.com/home/post/p-184497902",
        "title": "books that made me",
        "authors": "Substack",
        "abstract": "42 books from a lifetime of reading",
        "timestamp": "2026-02-11T14:01:35.241Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 98,
        "object_id": "paper:url.75A89B95",
        "created_at": "2026-02-11T14:01:35+00:00",
        "updated_at": "2026-02-11T14:01:56+00:00",
        "version": 1
      }
    },
    "paper:url.2E31D346": {
      "data": {
        "sourceId": "url",
        "paperId": "2E31D346",
        "url": "https://papercode.in/",
        "title": "PaperCode",
        "authors": "",
        "abstract": "Master ML papers by implementing them from scratch.",
        "timestamp": "2026-02-11T15:14:11.098Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 99,
        "object_id": "paper:url.2E31D346",
        "created_at": "2026-02-11T15:14:11+00:00",
        "updated_at": "2026-02-11T15:14:36+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.12690": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.12690",
        "url": "https://arxiv.org/abs/2601.12690",
        "title": "\"Are we writing an advice column for Spock here?\" Understanding Stereotypes in AI Advice for Autistic Users",
        "authors": "Caleb Wohn, Buse \u00c7ar\u0131k, Xiaohan Ding, Sang Won Lee, Young-Ho Kim, Eugenia H. Rho",
        "abstract": "Autistic individuals sometimes disclose autism when asking LLMs for social advice, hoping for more personalized responses. However, they also recognize that these systems may reproduce stereotypes, raising uncertainty about the risks and benefits of disclosure. We conducted a mixed-methods study combining a large-scale LLM audit experiment with interviews involving 11 autistic participants. We developed a six-step pipeline operationalizing 12 documented autism stereotypes into decision-making scenarios framed as users requesting advice (e.g., \"Should I do A or B?\"). We generated 345,000 responses from six LLMs and measured how advice shifted when prompts disclosed autism versus when they did not. When autism was disclosed, LLMs disproportionately recommended avoiding stereotypically stressful situations, including social events, confrontations, new experiences, and romantic relationships. While some participants viewed this as affirming, others criticized it as infantilizing or undermining opportunities for growth. Our study illuminates how the intermingling of affirmation and stereotyping complicates the personalization of LLMs.",
        "timestamp": "2026-02-11T16:20:39.197Z",
        "rating": "novote",
        "publishedDate": "2026/01/19",
        "tags": [
          "Human-Computer Interaction (cs.HC)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 100,
        "object_id": "paper:arxiv.2601.12690",
        "created_at": "2026-02-11T16:20:39+00:00",
        "updated_at": "2026-02-11T16:21:07+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.12134": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.12134",
        "url": "https://arxiv.org/abs/2601.12134",
        "title": "Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and the Value of Human Partner in Collaborative Learning",
        "authors": "Taufiq Daryanto, Xiaohan Ding, Kaike Ping, Lance T. Wilhelm, Yan Chen, Chris Brown, Eugenia H. Rho",
        "abstract": "As AI assistance becomes embedded in programming practice, researchers have increasingly examined how these systems help learners generate code and work more efficiently. However, these studies often position AI as a replacement for human collaboration and overlook the social and learning-oriented aspects that emerge in collaborative programming. Our work introduces human-human-AI (HHAI) triadic programming, where an AI agent serves as an additional collaborator rather than a substitute for a human partner. Through a within-subjects study with 20 participants, we show that triadic collaboration enhances collaborative learning and social presence compared to the dyadic human-AI (HAI) baseline. In the triadic HHAI conditions, participants relied significantly less on AI-generated code in their work. This effect was strongest in the HHAI-shared condition, where participants had an increased sense of responsibility to understand AI suggestions before applying them. These findings demonstrate how triadic settings activate socially shared regulation of learning by making AI use visible and accountable to a human peer, suggesting that AI systems that augment rather than automate peer collaboration can better preserve the learning processes that collaborative programming relies on.",
        "timestamp": "2026-02-11T16:27:18.024Z",
        "rating": "novote",
        "publishedDate": "2026/01/17",
        "tags": [
          "Human-Computer Interaction (cs.HC)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 101,
        "object_id": "paper:arxiv.2601.12134",
        "created_at": "2026-02-11T16:27:18+00:00",
        "updated_at": "2026-02-11T16:27:44+00:00",
        "version": 1
      }
    },
    "paper:url.388A6126": {
      "data": {
        "sourceId": "url",
        "paperId": "388A6126",
        "url": "https://github.com/qwibitai/nanoclaw",
        "title": "qwibitai/nanoclaw: A lightweight alternative to Clawdbot / OpenClaw that runs in Apple containers for security. Connects to WhatsApp, has memory, scheduled jobs, and runs directly on Anthropic's Agents SDK",
        "authors": "",
        "abstract": "A lightweight alternative to Clawdbot / OpenClaw that runs in Apple containers for security. Connects to WhatsApp, has memory, scheduled jobs, and runs directly on Anthropic&#39;s Agents SDK - qwib...",
        "timestamp": "2026-02-11T16:36:20.138Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 102,
        "object_id": "paper:url.388A6126",
        "created_at": "2026-02-11T16:36:20+00:00",
        "updated_at": "2026-02-11T16:36:46+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.23840": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.23840",
        "url": "https://arxiv.org/abs/2505.23840",
        "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues",
        "authors": "Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu, Jinho D. Choi",
        "abstract": "Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce SYCON Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying SYCON Bench to 17 LLMs across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the model's ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the user's underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8% in debate scenario. We release our code and data at this https URL.",
        "timestamp": "2026-02-11T16:37:28.356Z",
        "rating": "novote",
        "publishedDate": "2025/05/28",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 103,
        "object_id": "paper:arxiv.2505.23840",
        "created_at": "2026-02-11T16:37:28+00:00",
        "updated_at": "2026-02-11T16:37:52+00:00",
        "version": 1
      }
    },
    "paper:url.15DF551C": {
      "data": {
        "sourceId": "url",
        "paperId": "15DF551C",
        "url": "https://x.com/C_Hendrick/status/2021014941717917797",
        "title": "Carl Hendrick on X: \"Why haven't we have a great AI novel yet? \nBecause LLMs are compression engines trained on the statistical regularities of human text. When generating stories, they produce a kind of average of all stories in the training distribution. This is why LLM fiction tends to be so\" / X",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-11T21:35:30.320Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 105,
        "object_id": "paper:url.15DF551C",
        "created_at": "2026-02-11T21:35:30+00:00",
        "updated_at": "2026-02-11T21:35:49+00:00",
        "version": 1
      }
    },
    "paper:url.46A0BBB7": {
      "data": {
        "sourceId": "url",
        "paperId": "46A0BBB7",
        "url": "https://x.com/adamjohnsonCHI/status/2021619507010388269",
        "title": "Adam Johnson on X: \"It absolutely cannot \u201ccompare bodies of literature at high levels of abstraction\u201d or \u201cwrite entire books\u201d of any quality. This is simply not true. It can produce summations of technical writing or highly derivative pastiche that is barely readable. This is just horseshit.\" / X",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-11T21:35:26.578Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 104,
        "object_id": "paper:url.46A0BBB7",
        "created_at": "2026-02-11T21:35:26+00:00",
        "updated_at": "2026-02-11T21:35:51+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.09770": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.09770",
        "url": "https://arxiv.org/abs/2510.09770",
        "title": "GOLD PANNING: Iterative Bayesian Signal Anchoring for Many-Document Needle-in-Haystack Reasoning",
        "authors": "Adam Byerly, Daniel Khashabi",
        "abstract": "Large language models (LLMs) exhibit pronounced position bias in long-context needle-in-haystack problems, systematically prioritizing the location of information over its relevance. While current mitigations rely on white-box access, this is effectively impossible for many state-of-the-art models. We introduce GOLD PANNING, a black-box Bayesian framework that performs inference-time active search over long contexts by (i) reordering documents to concentrate high-belief items in highly diagnostic positions (signal anchoring) and (ii) updating beliefs over document relevance from model outputs. Unlike conventional active learning, which prioritizes uncertainty reduction, GOLD PANNING leverages anchoring -- once flagged, keep it in sight -- to preserve weak cues. We implement this using iterative assignment derived from the model's diagnosticity profile, which provably identifies a target among NN documents in O(logN)O(\\log N) rounds, ensuring scalability to many-document this http URL needle-in-a-haystack retrieval and long-context QA, GOLD PANNING matches Permutation Self-Consistency's target identification with 30\u2212\u22126530--65% fewer queries and remains effective under calibration mismatch, suggesting coarse positional ordering drives performance gains. These results demonstrate that inherent model biases need not be failures, but can be used as tools for control.",
        "timestamp": "2026-02-11T22:58:51.386Z",
        "rating": "novote",
        "publishedDate": "2025/10/10",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 106,
        "object_id": "paper:arxiv.2510.09770",
        "created_at": "2026-02-11T22:58:51+00:00",
        "updated_at": "2026-02-11T22:59:09+00:00",
        "version": 1
      }
    },
    "paper:url.1160CF8B": {
      "data": {
        "sourceId": "url",
        "paperId": "1160CF8B",
        "url": "https://zorazrw.github.io/files/position-haicode.pdf",
        "title": "1160CF8B",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-11T23:03:24.896Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 107,
        "object_id": "paper:url.1160CF8B",
        "created_at": "2026-02-11T23:03:25+00:00",
        "updated_at": "2026-02-11T23:03:45+00:00",
        "version": 1
      }
    },
    "paper:url-misc.2D252E1F": {
      "data": {
        "sourceId": "url-misc",
        "paperId": "2D252E1F",
        "url": "https://lily834454.substack.com/p/letter-to-my-phd-advisor",
        "title": "Letter to My PhD Advisor",
        "authors": "Lily",
        "abstract": "Farewell academia (for now)",
        "timestamp": "2026-02-11T23:27:07.216Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 108,
        "object_id": "paper:url-misc.2D252E1F",
        "created_at": "2026-02-11T23:27:07+00:00",
        "updated_at": "2026-02-11T23:27:29+00:00",
        "version": 1
      }
    },
    "paper:url.667FE82F": {
      "data": {
        "sourceId": "url",
        "paperId": "667FE82F",
        "url": "https://bsky.app/profile/markriedl.bsky.social/post/3mekkln7ws22z",
        "title": "Mark Riedl (@markriedl.bsky.social)",
        "authors": "",
        "abstract": "This is why I always verify the results when I use AI",
        "timestamp": "2026-02-12T01:21:39.887Z",
        "rating": "novote",
        "publishedDate": "2026-02-11T03:40:49.154Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 109,
        "object_id": "paper:url.667FE82F",
        "created_at": "2026-02-12T01:21:40+00:00",
        "updated_at": "2026-02-12T01:21:58+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2410.02428": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2410.02428",
        "url": "https://arxiv.org/abs/2410.02428",
        "title": "Collective Critics for Creative Story Generation",
        "authors": "Minwook Bae, Hyounghun Kim",
        "abstract": "Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers' interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing.",
        "timestamp": "2026-02-12T14:01:38.725Z",
        "rating": "novote",
        "publishedDate": "2024/10/03",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 113,
        "object_id": "paper:arxiv.2410.02428",
        "created_at": "2026-02-12T14:01:39+00:00",
        "updated_at": "2026-02-12T14:02:02+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.07149": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.07149",
        "url": "https://arxiv.org/abs/2601.07149",
        "title": "Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling",
        "authors": "Zhaoyan Li, Hang Lei, Yujia Wang, Lanbo Liu, Hao Liu, Liang Yu",
        "abstract": "While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68\\% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.",
        "timestamp": "2026-02-12T14:01:28.997Z",
        "rating": "novote",
        "publishedDate": "2026/01/12",
        "tags": [
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 112,
        "object_id": "paper:arxiv.2601.07149",
        "created_at": "2026-02-12T14:01:29+00:00",
        "updated_at": "2026-02-12T14:01:48+00:00",
        "version": 1
      }
    },
    "paper:url.6132AA67": {
      "data": {
        "sourceId": "url",
        "paperId": "6132AA67",
        "url": "https://www.mdpi.com/2076-3417/15/6/2971",
        "title": "Evaluating Creativity: Can LLMs Be Good Evaluators in Creative Writing Tasks?",
        "authors": "Kim, Sungeun, Oh, Dongsuk",
        "abstract": "The evaluation of creative writing has long been a complex and subjective process, made even more intriguing by the rise of advanced Artificial Intelligence (AI) tools like Large Language Models (LLMs). This study evaluates the potential of LLMs as reliable and consistent evaluators of creative texts, directly comparing their performance with traditional human evaluations. The analysis focuses on key creative criteria, including fluency, flexibility, elaboration, originality, usefulness, and specific creativity strategies. Results demonstrate that LLMs provide consistent and objective evaluations, achieving higher Inter-Annotator Agreement (IAA) compared with human evaluators. However, LLMs face limitations in recognizing nuanced, culturally specific, and context-dependent aspects of creativity. Conversely, human evaluators, despite lower consistency and higher subjectivity, exhibit strengths in capturing deeper contextual insights. These findings highlight the need for the further refinement of LLMs to address the complexities of creative writing evaluation.",
        "timestamp": "2026-02-12T14:01:12.100Z",
        "rating": "novote",
        "publishedDate": "2025-03-10",
        "tags": [
          "large language models (LLMs) evaluation"
        ],
        "doi": "10.3390/app15062971",
        "journalName": "Applied Sciences 2025, Vol. 15, Page 2971",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 111,
        "object_id": "paper:url.6132AA67",
        "created_at": "2026-02-12T14:01:12+00:00",
        "updated_at": "2026-02-12T14:01:36+00:00",
        "version": 1
      }
    },
    "paper:acl-anthology.2025.acl-long.1254": {
      "data": {
        "sourceId": "acl-anthology",
        "paperId": "2025.acl-long.1254",
        "url": "https://aclanthology.org/2025.acl-long.1254/",
        "title": "Help Me Write a Story: Evaluating LLMs\u2019 Ability to Generate Writing Feedback",
        "authors": "Hannah Rashkin, Elizabeth Clark, Fantine Huot, Mirella Lapata",
        "abstract": "AbstractCan LLMs provide support to creative writers by giving meaningful writing feedback? In this paper, we explore the challenges and limitations of model-generated writing feedback by defining a new task, dataset, and evaluation frameworks. To study model performance in a controlled manner, we present a novel test set of 1,300 stories that we corrupted to intentionally introduce writing issues. We study the performance of commonly used LLMs in this task with both automatic and human evaluation metrics. Our analysis shows that current models have strong out-of-the-box behavior in many respects\u2014providing specific and mostly accurate writing feedback. However, models often fail to identify the biggest writing issue in the story and to correctly decide when to offer critical vs. positive feedback.",
        "timestamp": "2026-02-12T14:00:59.425Z",
        "rating": "novote",
        "publishedDate": "2025/7",
        "tags": [],
        "doi": "10.18653/v1/2025.acl-long.1254",
        "journalName": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 110,
        "object_id": "paper:acl-anthology.2025.acl-long.1254",
        "created_at": "2026-02-12T14:00:59+00:00",
        "updated_at": "2026-02-12T14:07:38+00:00",
        "version": 1
      }
    },
    "paper:acl-anthology.2021.humeval-1.3": {
      "data": {
        "sourceId": "acl-anthology",
        "paperId": "2021.humeval-1.3",
        "url": "https://aclanthology.org/2021.humeval-1.3/",
        "title": "Trading Off Diversity and Quality in Natural Language Generation",
        "authors": "Hugh Zhang, Daniel Duckworth, Daphne Ippolito, Arvind Neelakantan",
        "abstract": "AbstractFor open-ended language generation tasks such as storytelling or dialogue, choosing the right decoding algorithm is vital for controlling the tradeoff between generation quality and diversity. However, there presently exists no consensus on which decoding procedure is best or even the criteria by which to compare them. In this paper, we cast decoding as a tradeoff between response quality and diversity, and we perform the first large-scale evaluation of decoding methods along the entire quality-diversity spectrum. Our experiments confirm the existence of the likelihood trap: the counter-intuitive observation that high likelihood sequences are often surprisingly low quality. We also find that when diversity is a priority, all methods perform similarly, but when quality is viewed as more important, nucleus sampling (Holtzman et al., 2019) outperforms all other evaluated decoding algorithms.",
        "timestamp": "2026-02-12T14:29:06.349Z",
        "rating": "novote",
        "publishedDate": "2021/4",
        "tags": [],
        "doi": "",
        "journalName": "Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval)",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 114,
        "object_id": "paper:acl-anthology.2021.humeval-1.3",
        "created_at": "2026-02-12T14:29:06+00:00",
        "updated_at": "2026-02-12T14:29:33+00:00",
        "version": 1
      }
    },
    "paper:url.431C4322": {
      "data": {
        "sourceId": "url",
        "paperId": "431C4322",
        "url": "https://www.openculture.com/2025/09/the-carl-sagan-baloney-detection-kit.html",
        "title": "Carl Sagan\u2019s Baloney Detection Kit: Tools for Thinking Critically & Knowing Pseudoscience When You See It",
        "authors": "",
        "abstract": "Though he died too young, Carl Sagan left behind an impressively large body of work, including more than 600 scientific papers and more than 20 books.",
        "timestamp": "2026-02-12T14:56:47.631Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 118,
        "object_id": "paper:url.431C4322",
        "created_at": "2026-02-12T14:56:47+00:00",
        "updated_at": "2026-02-12T14:57:11+00:00",
        "version": 1
      }
    },
    "paper:url.D283675": {
      "data": {
        "sourceId": "url",
        "paperId": "D283675",
        "url": "https://ntietz.com/blog/using-an-engineering-notebook/",
        "title": "Using an engineering notebook | nicole@web",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-12T14:56:37.123Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 117,
        "object_id": "paper:url.D283675",
        "created_at": "2026-02-12T14:56:37+00:00",
        "updated_at": "2026-02-12T14:56:57+00:00",
        "version": 1
      }
    },
    "paper:url.193FF853": {
      "data": {
        "sourceId": "url",
        "paperId": "193FF853",
        "url": "https://www.vangemert.dev/blog/nothing",
        "title": "\"Nothing\" is the secret to structuring your work",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-12T14:56:33.261Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 116,
        "object_id": "paper:url.193FF853",
        "created_at": "2026-02-12T14:56:33+00:00",
        "updated_at": "2026-02-12T14:56:59+00:00",
        "version": 1
      }
    },
    "paper:url.5873061B": {
      "data": {
        "sourceId": "url",
        "paperId": "5873061B",
        "url": "https://blog.can.ac/2026/02/12/the-harness-problem/",
        "title": "I Improved 15 LLMs at Coding in One Afternoon. Only the Harness Changed.",
        "authors": "",
        "abstract": "Cross-posted from X / @_can1357 In fact only the edit tool changed. That\u2019s it.\n0x0: The Wrong Question The conversation right now is almost entirely about \u2026",
        "timestamp": "2026-02-12T14:56:10.447Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 115,
        "object_id": "paper:url.5873061B",
        "created_at": "2026-02-12T14:56:10+00:00",
        "updated_at": "2026-02-12T14:56:32+00:00",
        "version": 1
      }
    },
    "paper:url.5146ACEF": {
      "data": {
        "sourceId": "url",
        "paperId": "5146ACEF",
        "url": "https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview",
        "title": "Prompt engineering overview",
        "authors": "",
        "abstract": "Claude API Documentation",
        "timestamp": "2026-02-12T16:53:26.293Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 119,
        "object_id": "paper:url.5146ACEF",
        "created_at": "2026-02-12T16:53:26+00:00",
        "updated_at": "2026-02-12T16:53:47+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2503.03703": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2503.03703",
        "url": "https://arxiv.org/abs/2503.03703",
        "title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches",
        "authors": "Hiroyuki Deguchi, Go Kamoda, Yusuke Matsushita, Chihiro Taguchi, Kohei Suenaga, Masaki Waga, Sho Yokoi",
        "abstract": "Researchers and practitioners in natural language processing and computational linguistics frequently observe and analyze the real language usage in large-scale corpora. For that purpose, they often employ off-the-shelf pattern-matching tools, such as grep, and keyword-in-context concordancers, which is widely used in corpus linguistics for gathering examples. Nonetheless, these existing techniques rely on surface-level string matching, and thus they suffer from the major limitation of not being able to handle orthographic variations and paraphrasing -- notable and common phenomena in any natural language. In addition, existing continuous approaches such as dense vector search tend to be overly coarse, often retrieving texts that are unrelated but share similar topics. Given these challenges, we propose a novel algorithm that achieves \\emph{soft} (or semantic) yet efficient pattern matching by relaxing a surface-level matching with word embeddings. Our algorithm is highly scalable with respect to the size of the corpus text utilizing inverted indexes. We have prepared an efficient implementation, and we provide an accessible web tool. Our experiments demonstrate that the proposed method (i) can execute searches on billion-scale corpora in less than a second, which is comparable in speed to surface-level string matching and dense vector search; (ii) can extract harmful instances that semantically match queries from a large set of English and Japanese Wikipedia articles; and (iii) can be effectively applied to corpus-linguistic analyses of Latin, a language with highly diverse inflections.",
        "timestamp": "2026-02-12T18:01:12.104Z",
        "rating": "novote",
        "publishedDate": "2025/03/05",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 120,
        "object_id": "paper:arxiv.2503.03703",
        "created_at": "2026-02-12T18:01:12+00:00",
        "updated_at": "2026-02-12T18:01:38+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.10908": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.10908",
        "url": "https://arxiv.org/abs/2602.10908",
        "title": "SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora",
        "authors": "Masataka Yoneda, Yusuke Matsushita, Go Kamoda, Kohei Suenaga, Takuya Akiba, Masaki Waga, Sho Yokoi",
        "abstract": "We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.",
        "timestamp": "2026-02-12T18:02:53.134Z",
        "rating": "novote",
        "publishedDate": "2026/02/11",
        "tags": [
          "Computation and Language (cs.CL)",
          "Machine Learning (cs.LG)",
          "Machine Learning (stat.ML)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 121,
        "object_id": "paper:arxiv.2602.10908",
        "created_at": "2026-02-12T18:02:53+00:00",
        "updated_at": "2026-02-12T18:03:18+00:00",
        "version": 1
      }
    },
    "paper:url.42F10F94": {
      "data": {
        "sourceId": "url",
        "paperId": "42F10F94",
        "url": "https://platform.claude.com/docs/en/resources/prompt-library/library",
        "title": "Prompt Library",
        "authors": "",
        "abstract": "Claude API Documentation",
        "timestamp": "2026-02-12T18:55:24.309Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 122,
        "object_id": "paper:url.42F10F94",
        "created_at": "2026-02-12T18:55:24+00:00",
        "updated_at": "2026-02-12T18:56:03+00:00",
        "version": 1
      }
    },
    "paper:url.6B2919C7": {
      "data": {
        "sourceId": "url",
        "paperId": "6B2919C7",
        "url": "https://monosketch.io/",
        "title": "MonoSketch - Unleash your ideas with ASCII",
        "authors": "",
        "abstract": "MonoSketch is a powerful ASCII sketching and diagramming app that lets you effortlessly transform your ideas into visually stunning designs.",
        "timestamp": "2026-02-13T13:42:54.428Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 123,
        "object_id": "paper:url.6B2919C7",
        "created_at": "2026-02-13T13:42:54+00:00",
        "updated_at": "2026-02-13T13:43:14+00:00",
        "version": 1
      }
    },
    "paper:url.61759AF0": {
      "data": {
        "sourceId": "url",
        "paperId": "61759AF0",
        "url": "https://www.alphaxiv.org/abs/intrinsic-credit-assignment",
        "title": "Intrinsic Credit Assignment for Long Horizon Interaction | alphaXiv",
        "authors": "",
        "abstract": "View recent discussion. Abstract: This work proposes \u2206Belief-RL, a method that leverages a language model's own intrinsic beliefs to reward intermediate progress in long horizon tasks. The method utilizes the change in the probability an agent assigns to the target solution for credit assignment, and teaches information-seeking capabilities that consistently outperform purely outcome-based rewards for RL.",
        "timestamp": "2026-02-13T19:19:14.858Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "alphaxiv",
          "arxiv",
          "forum",
          "discussion",
          "explore",
          "trending papers"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 124,
        "object_id": "paper:url.61759AF0",
        "created_at": "2026-02-13T19:19:15+00:00",
        "updated_at": "2026-02-13T19:19:40+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.12005": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.12005",
        "url": "https://arxiv.org/abs/2602.12005",
        "title": "LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss",
        "authors": "Szilvia Ujv\u00e1ry, Louis B\u00e9thune, Pierre Ablin, Jo\u00e3o Monteiro, Marco Cuturi, Michael Kirchhof",
        "abstract": "Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \\emph{which tokens an SLM can and should learn} during pretraining, versus \\emph{which ones it should delegate} via a \\texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \\emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \\texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.",
        "timestamp": "2026-02-13T19:31:06.668Z",
        "rating": "novote",
        "publishedDate": "2026/02/12",
        "tags": [
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 125,
        "object_id": "paper:arxiv.2602.12005",
        "created_at": "2026-02-13T19:31:06+00:00",
        "updated_at": "2026-02-13T19:31:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.12237": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.12237",
        "url": "https://arxiv.org/abs/2602.12237",
        "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
        "authors": "Mayee F. Chen, Tyler Murray, David Heineman, Matt Jordan, Hannaneh Hajishirzi, Christopher R\u00e9, Luca Soldaini, Kyle Lo",
        "abstract": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design choices across existing methods lack justification or consensus and overlook practical issues like data constraints. We conduct a comprehensive empirical study of this space, identifying which design choices lead to a strong mixing method. Second, in practice, the domain set evolves throughout LM development as datasets are added, removed, partitioned, and revised -- a problem setting largely unaddressed by existing works, which assume fixed domains. We study how to efficiently recompute the mixture after the domain set is updated, leveraging information from past mixtures. We introduce mixture reuse, a mechanism that reuses existing ratios and recomputes ratios only for domains affected by the update. Over a sequence of five domain-set updates mirroring real-world LM development, mixture reuse matches the performance of fully recomputing the mix after each update with 74% less compute and improves over training without mixing by 11.6% on downstream tasks.",
        "timestamp": "2026-02-13T19:41:00.485Z",
        "rating": "novote",
        "publishedDate": "2026/02/12",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 126,
        "object_id": "paper:arxiv.2602.12237",
        "created_at": "2026-02-13T19:41:00+00:00",
        "updated_at": "2026-02-13T19:41:38+00:00",
        "version": 1
      }
    },
    "paper:url.2B4B4F76": {
      "data": {
        "sourceId": "url",
        "paperId": "2B4B4F76",
        "url": "https://code.claude.com/docs/en/how-claude-code-works",
        "title": "How Claude Code works - Claude Code Docs",
        "authors": "",
        "abstract": "Understand the agentic loop, built-in tools, and how Claude Code interacts with your project.",
        "timestamp": "2026-02-13T19:44:54.970Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 127,
        "object_id": "paper:url.2B4B4F76",
        "created_at": "2026-02-13T19:44:55+00:00",
        "updated_at": "2026-02-13T19:45:41+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2510.12487": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2510.12487",
        "url": "https://arxiv.org/abs/2510.12487",
        "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding",
        "authors": "Evgeniy Glukhov, Michele Conti, Egor Bogomolov, Yaroslav Golubev, Alexander Bezzubov",
        "abstract": "Reliable handling of code diffs is central to agents that edit and refactor repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff understanding with three supervised tasks: apply (old code ++ diff \u2192\\rightarrow new code), anti-apply (new code \u2212- diff \u2192\\rightarrow old code), and diff generation (new code \u2212- old code \u2192\\rightarrow diff). Instances in the benchmark are triples \u27e8old code,new code,diff\u27e9\\langle \\textit{old code}, \\textit{new code}, \\textit{diff} \\rangle drawn from real commits in CommitPackFT, paired with automatic metrics and a clear evaluation protocol. We use the benchmark to do a focused empirical study of the unified diff format and run a cross-format comparison of different diff representations. Our findings reveal that different formats should be used depending on the use case and model size. For example, representing diffs in search-replace format performs best for larger models across most tasks, while structured udiff variants offer similar but slightly weaker performance. In contrast, smaller open models benefit little from any formatting choice. The Diff-XYZ benchmark is a reusable foundation for assessing and improving diff handling in LLMs that can aid future development of diff formats and models editing code. The dataset is published on HuggingFace Hub: this https URL.",
        "timestamp": "2026-02-13T19:51:46.111Z",
        "rating": "novote",
        "publishedDate": "2025/10/14",
        "tags": [
          "Software Engineering (cs.SE)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 128,
        "object_id": "paper:arxiv.2510.12487",
        "created_at": "2026-02-13T19:51:46+00:00",
        "updated_at": "2026-02-13T19:52:09+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2511.04486": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2511.04486",
        "url": "https://arxiv.org/abs/2511.04486",
        "title": "EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits",
        "authors": "Wayne Chi, Valerie Chen, Ryan Shar, Aditya Mittal, Jenny Liang, Wei-Lin Chiang, Anastasios Nikolas Angelopoulos, Ion Stoica, Graham Neubig, Ameet Talwalkar, Chris Donahue",
        "abstract": "Instructed code editing, where LLMs directly modify a developer's existing code based on a user instruction, is becoming a widely used interaction mode in AI coding assistants. However, few benchmarks directly evaluate this capability and current datasets often rely on artificial sources. We introduce EDIT-Bench, a benchmark for evaluating LLM code editing capabilities grounded in real-world usage, i.e., user instructions and code contexts collected in the wild. EDIT-Bench comprises of 540 problems, multiple natural and programming languages, and a diverse set of real-world use cases, ranging from resolving errors to adding features. EDIT-Bench introduces context-dependent problems that require the model to understand code context, highlighted code, and cursor position in addition to the user instruction. We evaluate 40 diverse LLMs and observe that EDIT-Bench is a challenging set of problems where only 1 model scores over 60%. We find that model performance varies across different categories of user instructions. Further, we find that varying levels of contextual information greatly affect task success rate, with performance varying up to 11%, indicating the importance of evaluating with realistic context.",
        "timestamp": "2026-02-13T19:52:29.324Z",
        "rating": "novote",
        "publishedDate": "2025/11/06",
        "tags": [
          "Software Engineering (cs.SE)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 129,
        "object_id": "paper:arxiv.2511.04486",
        "created_at": "2026-02-13T19:52:29+00:00",
        "updated_at": "2026-02-13T19:52:52+00:00",
        "version": 1
      }
    },
    "paper:url.78223B6A": {
      "data": {
        "sourceId": "url",
        "paperId": "78223B6A",
        "url": "https://humanlm.stanford.edu/index.html",
        "title": "HumanLM",
        "authors": "",
        "abstract": "HumanLM: Simulating Users with State Alignment Beats Response Imitation",
        "timestamp": "2026-02-13T20:07:18.027Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 130,
        "object_id": "paper:url.78223B6A",
        "created_at": "2026-02-13T20:07:18+00:00",
        "updated_at": "2026-02-13T20:07:39+00:00",
        "version": 1
      }
    },
    "paper:nature.s42256-025-01169-6": {
      "data": {
        "sourceId": "nature",
        "paperId": "s42256-025-01169-6",
        "url": "https://www.nature.com/articles/s42256-025-01169-6",
        "title": "When large language models are reliable for judging empathic communication",
        "authors": "Kumar, Aakriti",
        "abstract": "Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? Here we investigate this question by comparing how experts, crowdworkers and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations and 3,150 LLM annotations, we assess interrater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks\u2019 subcomponents depending on their clarity, complexity and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions. Kumar et al. show that large language models (LLMs) nearly match expert reliability and outperform laypeople when assessing empathic communication across multiple frameworks. The performance of both LLMs and experts depends on clear and specific evaluation criteria.",
        "timestamp": "2026-02-13T20:08:36.471Z",
        "rating": "novote",
        "publishedDate": "2026-02-11",
        "tags": [
          "Business and management"
        ],
        "doi": "10.1038/s42256-025-01169-6",
        "journalName": "Nature Machine Intelligence 2026",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 131,
        "object_id": "paper:nature.s42256-025-01169-6",
        "created_at": "2026-02-13T20:08:36+00:00",
        "updated_at": "2026-02-13T20:09:11+00:00",
        "version": 1
      }
    },
    "paper:url.6F9EE56B": {
      "data": {
        "sourceId": "url",
        "paperId": "6F9EE56B",
        "url": "https://m-joon-ixix.github.io/assets/files/Illusions-of-the-Gold-Standard_ARR-Jan2026.pdf",
        "title": "6F9EE56B",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-13T22:28:07.611Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "pdf"
      },
      "meta": {
        "issue_number": 132,
        "object_id": "paper:url.6F9EE56B",
        "created_at": "2026-02-13T22:28:07+00:00",
        "updated_at": "2026-02-13T22:28:37+00:00",
        "version": 1
      }
    },
    "paper:url-misc.4E1986C7": {
      "data": {
        "sourceId": "url-misc",
        "paperId": "4E1986C7",
        "url": "https://gvrkiran.substack.com/p/ai-agents-and-academia",
        "title": "AI Agents and Academia",
        "authors": "Kiran Garimella",
        "abstract": "Two years ago, when people talked about AI agents, most imagined browser automation: systems that browse the web, book flights, find deals.",
        "timestamp": "2026-02-14T16:56:19.120Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 133,
        "object_id": "paper:url-misc.4E1986C7",
        "created_at": "2026-02-14T16:56:19+00:00",
        "updated_at": "2026-02-14T16:56:40+00:00",
        "version": 1
      }
    },
    "paper:url-misc.180DDC29": {
      "data": {
        "sourceId": "url-misc",
        "paperId": "180DDC29",
        "url": "https://empiricrafting.substack.com/p/why-cant-your-ai-agent-book-a-flight",
        "title": "Why Can\u2019t Your AI Agent Book a Flight? ",
        "authors": "Andrey Fradkin",
        "abstract": "The Argument for Facilitating and Legally Protecting Agentic Access Online",
        "timestamp": "2026-02-15T00:12:10.019Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 134,
        "object_id": "paper:url-misc.180DDC29",
        "created_at": "2026-02-15T00:12:10+00:00",
        "updated_at": "2026-02-15T00:12:31+00:00",
        "version": 1
      }
    },
    "paper:url.11394F17": {
      "data": {
        "sourceId": "url",
        "paperId": "11394F17",
        "url": "https://ooh.directory/",
        "title": "ooh.directory: a place to find good blogs that interest you",
        "authors": "",
        "abstract": "A collection of 2,380 blogs about every topic",
        "timestamp": "2026-02-15T01:13:54.823Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "blogs",
          "weblogs"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 135,
        "object_id": "paper:url.11394F17",
        "created_at": "2026-02-15T01:13:55+00:00",
        "updated_at": "2026-02-15T01:14:11+00:00",
        "version": 1
      }
    },
    "paper:url.3E457B80": {
      "data": {
        "sourceId": "url",
        "paperId": "3E457B80",
        "url": "https://www.theatlantic.com/family/2026/01/friend-group-loneliness/685528/",
        "title": "The Friend-Group Fallacy",
        "authors": "Jenny Singer",
        "abstract": "Many people yearn for a crew, but having one is not actually the norm.",
        "timestamp": "2026-02-15T02:23:05.133Z",
        "rating": "novote",
        "publishedDate": "2026-01-07T14:48:15Z",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 136,
        "object_id": "paper:url.3E457B80",
        "created_at": "2026-02-15T02:23:05+00:00",
        "updated_at": "2026-02-15T02:23:25+00:00",
        "version": 1
      }
    },
    "paper:url.2FFAB8CF": {
      "data": {
        "sourceId": "url",
        "paperId": "2FFAB8CF",
        "url": "https://nichehunt.app/blog/ai-going-to-kill-app-subscriptions",
        "title": "Niche Hunt \u2014 App ideas people actually want",
        "authors": "",
        "abstract": "Curated niche app opportunities from Reddit, scored by difficulty and demand.",
        "timestamp": "2026-02-15T15:48:59.010Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [
          "app ideas",
          "niche apps",
          "mobile development",
          "indie hacker",
          "startup ideas"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 137,
        "object_id": "paper:url.2FFAB8CF",
        "created_at": "2026-02-15T15:48:59+00:00",
        "updated_at": "2026-02-15T15:49:18+00:00",
        "version": 1
      }
    },
    "paper:nature.s41562-026-02420-9": {
      "data": {
        "sourceId": "nature",
        "paperId": "s41562-026-02420-9",
        "url": "https://www.nature.com/articles/s41562-026-02420-9?utm_source=twitter&utm_medium=social&utm_campaign=nathumbehav",
        "title": "Why artificial intelligence detectors could penalize academic writing",
        "authors": "Hu, Bo",
        "abstract": "Writing produced using artificial intelligence is becoming more common in academia, which has prompted institutions to look for ways to detect it. Bo Hu warns that an overreliance on fixed linguistic markers may push scholars to flatten their writing to avoid claims it is generated by artificial intelligence.",
        "timestamp": "2026-02-16T17:34:14.989Z",
        "rating": "novote",
        "publishedDate": "2026-02-16",
        "tags": [
          "Authorship"
        ],
        "doi": "10.1038/s41562-026-02420-9",
        "journalName": "Nature Human Behaviour 2026",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 138,
        "object_id": "paper:nature.s41562-026-02420-9",
        "created_at": "2026-02-16T17:34:15+00:00",
        "updated_at": "2026-02-16T17:34:34+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.12670": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.12670",
        "url": "https://arxiv.org/abs/2602.12670",
        "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks",
        "authors": "Xiangyi Li, Wenbo Chen, Yimin Liu, Shenghan Zheng, Xiaokun Chen, Yifeng He, Yubo Li, Bingran You, Haotian Shen, Jiankai Sun, Shuyi Wang, Qunhong Zeng, Di Wang, Xuandong Zhao, Yuanli Wang, Roey Ben Chaim, Zonglin Di, Yipeng Gao, Junwei He, Yizhuo He, Liqiang Jing, Luyang Kong, Xin Lan, Jiachen Li, Songlin Li, Yijiang Li, Yueqian Lin, Xinyi Liu, Xuanqing Liu, Haoran Lyu, Ze Ma, Bowei Wang, Runhui Wang, Tianyu Wang, Wengao Ye, Yue Zhang, Hanwen Xing, Yiqi Xue, Steven Dillmann, Han-chung Lee",
        "abstract": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.",
        "timestamp": "2026-02-17T02:33:11.601Z",
        "rating": "novote",
        "publishedDate": "2026/02/13",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 139,
        "object_id": "paper:arxiv.2602.12670",
        "created_at": "2026-02-17T02:33:11+00:00",
        "updated_at": "2026-02-17T02:33:30+00:00",
        "version": 1
      }
    },
    "paper:url.70714C22": {
      "data": {
        "sourceId": "url",
        "paperId": "70714C22",
        "url": "https://www.ericmshen.com/writing",
        "title": "ericmshen | writing",
        "authors": "",
        "abstract": "",
        "timestamp": "2026-02-17T22:52:37.694Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 140,
        "object_id": "paper:url.70714C22",
        "created_at": "2026-02-17T22:52:37+00:00",
        "updated_at": "2026-02-17T22:52:55+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2505.06120": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2505.06120",
        "url": "https://arxiv.org/abs/2505.06120",
        "title": "LLMs Get Lost In Multi-Turn Conversation",
        "authors": "Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville",
        "abstract": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.",
        "timestamp": "2026-02-17T23:21:02.711Z",
        "rating": "novote",
        "publishedDate": "2025/05/09",
        "tags": [
          "Computation and Language (cs.CL)",
          "Human-Computer Interaction (cs.HC)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 141,
        "object_id": "paper:arxiv.2505.06120",
        "created_at": "2026-02-17T23:21:02+00:00",
        "updated_at": "2026-02-17T23:21:22+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2512.14982": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2512.14982",
        "url": "https://arxiv.org/abs/2512.14982",
        "title": "Prompt Repetition Improves Non-Reasoning LLMs",
        "authors": "Yaniv Leviathan, Matan Kalman, Yossi Matias",
        "abstract": "When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.",
        "timestamp": "2026-02-17T23:22:15.905Z",
        "rating": "novote",
        "publishedDate": "2025/12/17",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 142,
        "object_id": "paper:arxiv.2512.14982",
        "created_at": "2026-02-17T23:22:16+00:00",
        "updated_at": "2026-02-17T23:22:33+00:00",
        "version": 1
      }
    },
    "paper:url.1D30B4AF": {
      "data": {
        "sourceId": "url",
        "paperId": "1D30B4AF",
        "url": "https://substack.com/home/post/p-186684830",
        "title": "The New Primitives",
        "authors": "Substack",
        "abstract": "As AI agents prepare to reshape commerce, four technical primitives may promise to fix the Internet\u2019s original sin: an economy built on unverifiable attention.",
        "timestamp": "2026-02-17T23:53:43.895Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 143,
        "object_id": "paper:url.1D30B4AF",
        "created_at": "2026-02-17T23:53:44+00:00",
        "updated_at": "2026-02-17T23:54:04+00:00",
        "version": 1
      }
    },
    "paper:url.4506DCD7": {
      "data": {
        "sourceId": "url",
        "paperId": "4506DCD7",
        "url": "https://asteriskmag.com/issues/12-books/can-you-just-do-things",
        "title": "Can You Just Do Things?\u2014Asterisk",
        "authors": "",
        "abstract": "A conversation about how agency is gained, used, and misused. And also love, drugs, and the Enneagram.",
        "timestamp": "2026-02-17T23:55:06.227Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 144,
        "object_id": "paper:url.4506DCD7",
        "created_at": "2026-02-17T23:55:06+00:00",
        "updated_at": "2026-02-17T23:55:30+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.13949": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.13949",
        "url": "https://www.arxiv.org/abs/2602.13949",
        "title": "Experiential Reinforcement Learning",
        "authors": "Taiwei Shi, Sihao Chen, Bowen Jiang, Linxin Song, Longqi Yang, Jieyu Zhao",
        "abstract": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.",
        "timestamp": "2026-02-19T21:07:13.691Z",
        "rating": "novote",
        "publishedDate": "2026/02/15",
        "tags": [
          "Machine Learning (cs.LG)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 145,
        "object_id": "paper:arxiv.2602.13949",
        "created_at": "2026-02-19T21:07:13+00:00",
        "updated_at": "2026-02-19T21:07:36+00:00",
        "version": 1
      }
    },
    "paper:url.B142443": {
      "data": {
        "sourceId": "url",
        "paperId": "B142443",
        "url": "https://surgehq.ai/blog",
        "title": "Blog | Surge AI",
        "authors": "",
        "abstract": "Our latest thoughts, notes, and insights from the post-training frontier.",
        "timestamp": "2026-02-20T00:30:50.867Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 146,
        "object_id": "paper:url.B142443",
        "created_at": "2026-02-20T00:30:51+00:00",
        "updated_at": "2026-02-20T00:31:09+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.15198": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.15198",
        "url": "https://arxiv.org/abs/2602.15198",
        "title": "Colosseum: Auditing Collusion in Cooperative Multi-Agent Systems",
        "authors": "Mason Nakamura, Abhinav Kumar, Saswat Das, Sahar Abdelnabi, Saaduddin Mahmud, Ferdinando Fioretto, Shlomo Zilberstein, Eugene Bagdasarian",
        "abstract": "Multi-agent systems, where LLM agents communicate through free-form language, enable sophisticated coordination for solving complex cooperative tasks. This surfaces a unique safety problem when individual agents form a coalition and \\emph{collude} to pursue secondary goals and degrade the joint objective. In this paper, we present Colosseum, a framework for auditing LLM agents' collusive behavior in multi-agent settings. We ground how agents cooperate through a Distributed Constraint Optimization Problem (DCOP) and measure collusion via regret relative to the cooperative optimum. Colosseum tests each LLM for collusion under different objectives, persuasion tactics, and network topologies. Through our audit, we show that most out-of-the-box models exhibited a propensity to collude when a secret communication channel was artificially formed. Furthermore, we discover ``collusion on paper'' when agents plan to collude in text but would often pick non-collusive actions, thus providing little effect on the joint task. Colosseum provides a new way to study collusion by measuring communications and actions in rich yet verifiable environments.",
        "timestamp": "2026-02-20T01:45:32.811Z",
        "rating": "novote",
        "publishedDate": "2026/02/16",
        "tags": [
          "Multiagent Systems (cs.MA)",
          "Artificial Intelligence (cs.AI)",
          "Computation and Language (cs.CL)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 147,
        "object_id": "paper:arxiv.2602.15198",
        "created_at": "2026-02-20T01:45:33+00:00",
        "updated_at": "2026-02-20T01:45:55+00:00",
        "version": 1
      }
    },
    "paper:url.12CF30A0": {
      "data": {
        "sourceId": "url",
        "paperId": "12CF30A0",
        "url": "https://eric-tramel.github.io/blog/2026-02-18-slop-guard/",
        "title": "Slop Guard: Prose Linter for AI-Assisted Writing",
        "authors": "",
        "abstract": "A rule-based prose linter that catches formulaic writing patterns LLMs fall into. No LLM judge, no API calls. Just compiled regex and an afternoon of calibration against human reference texts.",
        "timestamp": "2026-02-20T02:57:04.106Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 148,
        "object_id": "paper:url.12CF30A0",
        "created_at": "2026-02-20T02:57:04+00:00",
        "updated_at": "2026-02-20T02:57:28+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2601.17569": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2601.17569",
        "url": "https://arxiv.org/abs/2601.17569",
        "title": "Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations",
        "authors": "Alireza Salemi, Hamed Zamani",
        "abstract": "Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce P3P^3, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In P3P^3, a large server-side model generates a sequence of kk draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that P3P^3 consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of 7.47.4% to 99% on average. Importantly, P3P^3 recovers 90.390.3% to 95.795.7% of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that P3P^3 preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage (1.51.5%--3.53.5%) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only 9.29.2% of the total tokens. These results demonstrate that P3P^3 provides a practical, effective solution for personalized generation with improved privacy.",
        "timestamp": "2026-02-20T15:39:02.470Z",
        "rating": "novote",
        "publishedDate": "2026/01/24",
        "tags": [
          "Computation and Language (cs.CL)",
          "Artificial Intelligence (cs.AI)",
          "Cryptography and Security (cs.CR)",
          "Information Retrieval (cs.IR)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 149,
        "object_id": "paper:arxiv.2601.17569",
        "created_at": "2026-02-20T15:39:02+00:00",
        "updated_at": "2026-02-20T15:39:26+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.16800": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.16800",
        "url": "https://arxiv.org/abs/2602.16800",
        "title": "Large-scale online deanonymization with LLMs",
        "authors": "Simon Lermen, Daniel Paleka, Joshua Swanson, Michael Aerni, Nicholas Carlini, Florian Tram\u00e8r",
        "abstract": "We show that large language models can be used to perform at-scale deanonymization. With full Internet access, our agent can re-identify Hacker News users and Anthropic Interviewer participants at high precision, given pseudonymous online profiles and conversations alone, matching what would take hours for a dedicated human investigator. We then design attacks for the closed-world setting. Given two databases of pseudonymous individuals, each containing unstructured text written by or about that individual, we implement a scalable attack pipeline that uses LLMs to: (1) extract identity-relevant features, (2) search for candidate matches via semantic embeddings, and (3) reason over top candidates to verify matches and reduce false positives. Compared to prior deanonymization work (e.g., on the Netflix prize) that required structured data or manual feature engineering, our approach works directly on raw user content across arbitrary platforms. We construct three datasets with known ground-truth data to evaluate our attacks. The first links Hacker News to LinkedIn profiles, using cross-platform references that appear in the profiles. Our second dataset matches users across Reddit movie discussion communities; and the third splits a single user's Reddit history in time to create two pseudonymous profiles to be matched. In each setting, LLM-based methods substantially outperform classical baselines, achieving up to 68% recall at 90% precision compared to near 0% for the best non-LLM method. Our results show that the practical obscurity protecting pseudonymous users online no longer holds and that threat models for online privacy need to be reconsidered.",
        "timestamp": "2026-02-20T15:41:03.081Z",
        "rating": "novote",
        "publishedDate": "2026/02/18",
        "tags": [
          "Cryptography and Security (cs.CR)",
          "Artificial Intelligence (cs.AI)",
          "Machine Learning (cs.LG)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 150,
        "object_id": "paper:arxiv.2602.16800",
        "created_at": "2026-02-20T15:41:03+00:00",
        "updated_at": "2026-02-20T15:41:25+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.16820": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.16820",
        "url": "https://arxiv.org/abs/2602.16820",
        "title": "AI-Mediated Feedback Improves Student Revisions: A Randomized Trial with FeedbackWriter in a Large Undergraduate Course",
        "authors": "Xinyi Lu, Kexin Phyllis Ju, Mitchell Dudley, Larissa Sano, Xu Wang",
        "abstract": "Despite growing interest in using LLMs to generate feedback on students' writing, little is known about how students respond to AI-mediated versus human-provided feedback. We address this gap through a randomized controlled trial in a large introductory economics course (N=354), where we introduce and deploy FeedbackWriter - a system that generates AI suggestions to teaching assistants (TAs) while they provide feedback on students' knowledge-intensive essays. TAs have the full capacity to adopt, edit, or dismiss the suggestions. Students were randomly assigned to receive either handwritten feedback from TAs (baseline) or AI-mediated feedback where TAs received suggestions from FeedbackWriter. Students revise their drafts based on the feedback, which is further graded. In total, 1,366 essays were graded using the system. We found that students receiving AI-mediated feedback produced significantly higher-quality revisions, with gains increasing as TAs adopted more AI suggestions. TAs found the AI suggestions useful for spotting gaps and clarifying rubrics.",
        "timestamp": "2026-02-20T15:41:39.316Z",
        "rating": "novote",
        "publishedDate": "2026/02/18",
        "tags": [
          "Human-Computer Interaction (cs.HC)",
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 151,
        "object_id": "paper:arxiv.2602.16820",
        "created_at": "2026-02-20T15:41:39+00:00",
        "updated_at": "2026-02-20T15:41:57+00:00",
        "version": 1
      }
    },
    "paper:arxiv.2602.16942": {
      "data": {
        "sourceId": "arxiv",
        "paperId": "2602.16942",
        "url": "https://arxiv.org/abs/2602.16942",
        "title": "SourceBench: Can AI Answers Reference Quality Web Sources?",
        "authors": "Hexi Jin, Stephen Liu, Yuheng Li, Simran Malik, Yiying Zhang",
        "abstract": "Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.",
        "timestamp": "2026-02-20T15:43:05.649Z",
        "rating": "novote",
        "publishedDate": "2026/02/18",
        "tags": [
          "Artificial Intelligence (cs.AI)"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 152,
        "object_id": "paper:arxiv.2602.16942",
        "created_at": "2026-02-20T15:43:05+00:00",
        "updated_at": "2026-02-20T15:43:27+00:00",
        "version": 1
      }
    },
    "paper:url.2734D91B": {
      "data": {
        "sourceId": "url",
        "paperId": "2734D91B",
        "url": "https://huggingface.co/collections/sergiopaniego/research-and-long-form-blog-posts",
        "title": "\ud83d\udcdd Research & Long-Form Blog Posts - a sergiopaniego Collection",
        "authors": "",
        "abstract": "In-depth technical articles and research pieces published by Hugging Face",
        "timestamp": "2026-02-20T16:19:40.348Z",
        "rating": "novote",
        "publishedDate": "",
        "tags": [],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 153,
        "object_id": "paper:url.2734D91B",
        "created_at": "2026-02-20T16:19:40+00:00",
        "updated_at": "2026-02-20T16:20:01+00:00",
        "version": 1
      }
    },
    "paper:url.77C263D5": {
      "data": {
        "sourceId": "url",
        "paperId": "77C263D5",
        "url": "https://gwern.net/blog/2025/good-ai-samples",
        "title": "Adding Bits Beats AI Slop",
        "authors": "Gwern",
        "abstract": "N/A",
        "timestamp": "2026-02-22T12:56:57.623Z",
        "rating": "novote",
        "publishedDate": "2025-03-23",
        "tags": [
          "ai/nn/diffusion/midjourney",
          "cs/algorithm/information",
          "reinforcement-learning/preference-learning"
        ],
        "doi": "",
        "journalName": "",
        "sourceType": "url"
      },
      "meta": {
        "issue_number": 154,
        "object_id": "paper:url.77C263D5",
        "created_at": "2026-02-22T12:56:57+00:00",
        "updated_at": "2026-02-22T12:57:16+00:00",
        "version": 1
      }
    }
  }
}